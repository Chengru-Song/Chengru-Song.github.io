<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="zh-CN"><generator uri="https://jekyllrb.com/" version="4.2.1">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" hreflang="zh-CN" /><updated>2024-01-29T15:59:35+08:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Chengru’s Blog</title><subtitle>A personal blog website for sharing of technology, reflection and branding. 
</subtitle><author><name>Chengru Song</name></author><entry><title type="html">【AI】Encoder-only Transformer</title><link href="http://localhost:4000/ai/ai_algorithms/2024/01/27/llm-encoder-only.html" rel="alternate" type="text/html" title="【AI】Encoder-only Transformer" /><published>2024-01-27T19:56:07+08:00</published><updated>2024-01-27T19:56:07+08:00</updated><id>http://localhost:4000/ai/ai_algorithms/2024/01/27/llm-encoder-only</id><content type="html" xml:base="http://localhost:4000/ai/ai_algorithms/2024/01/27/llm-encoder-only.html"><![CDATA[<p>基于Transformer的大语言模型共有三种架构，分别是Encoder-only Model，Encoder-Decoder Model和Decoder-only Model。</p>

<p>三者的本质区别：大模型的输出是文本还是Embedding。后者需要改模型结构才能适配其他下游任务。</p>

<ul>
  <li>Encoder-only： Input是Encoder Transformer，Output是Transformer结构的最后一层Hidden states，需要再加一层MLP才能适应到不同的下游任务。主要应用是训练高效的Embedding和各种文本分类问题。代表作：BERT。</li>
  <li>Encoder-Decoder：Input是语言，经过Transformer Encoder变成Embedding，再由Transformer Decoder解码Embedding转换回语言。代表作：FLAN-T5</li>
  <li>Decoder-only：用一个position Embedding layer替代Encoder，直接转化语言为Embedding，再用Transformer Decoder解码Embedding，输出语言，代表作：GPT。</li>
</ul>

<p>今天我们先看Encoder-only的代表作BERT的架构。</p>

<ol>
  <li>
    <p>模型架构</p>

    <ol>
      <li>multi-layer bidirectional Transformer encoder。</li>
    </ol>
  </li>
  <li>
    <p>数据形式</p>

    <ol>
      <li>Input：用CLS开头，SEP分割前后的文本</li>
      <li>Output：用C开头的Hidden states</li>
    </ol>
  </li>
  <li>
    <p>训练</p>

    <ol>
      <li>
        <p>Pretrain，80%情况下随机mask第i个token，然后用cross entropy loss来预测这个token</p>
      </li>
      <li>
        <p>train的时候用了one-hot来标记哪个log_prob是需要用来计算为cross entropy loss的</p>
      </li>
      <li>
        <p>还有一个任务是Next sentence prediction，计算下一个句子的log_prob</p>

        <p><img src="/assets/images/image-20240129153035295.png" alt="image-20240129153035295" /></p>
      </li>
      <li>
        <p>最后两个loss加起来了</p>

        <p><img src="/assets/images/image-20240129153048314.png" alt="image-20240129153048314" /></p>
      </li>
      <li>
        <p>Fine-tune: 把transformer的最后一层layer转化成一个适配下游任务的layer，不转不行，所以这个叫Encoder-only的Network。这里是一个classifier的实例，这里把最后一层Hidden states转化成了一个dense layer。
<img src="/assets/images/image-20240129153107209.png" alt="image-20240129153107209" /></p>
      </li>
    </ol>
  </li>
</ol>

<p>总结一下，Encoder-only的结构必须改模型结构才能使用到下游任务上，必须fine-tune，成本较高。下期讲Encoder-Decoder结构的代表作。</p>]]></content><author><name>Chengru Song</name></author><category term="[&quot;AI&quot;, &quot;AI_Algorithms&quot;]" /><category term="301-work-ai" /><summary type="html"><![CDATA[基于Transformer的大语言模型共有三种架构，分别是Encoder-only Model，Encoder-Decoder Model和Decoder-only Model。 三者的本质区别：大模型的输出是文本还是Embedding。后者需要改模型结构才能适配其他下游任务。 Encoder-only： Input是Encoder Transformer，Output是Transformer结构的最后一层Hidden states，需要再加一层MLP才能适应到不同的下游任务。主要应用是训练高效的Embedding和各种文本分类问题。代表作：BERT。 Encoder-Decoder：Input是语言，经过Transformer Encoder变成Embedding，再由Transformer Decoder解码Embedding转换回语言。代表作：FLAN-T5 Decoder-only：用一个position Embedding layer替代Encoder，直接转化语言为Embedding，再用Transformer Decoder解码Embedding，输出语言，代表作：GPT。 今天我们先看Encoder-only的代表作BERT的架构。 模型架构 multi-layer bidirectional Transformer encoder。 数据形式 Input：用CLS开头，SEP分割前后的文本 Output：用C开头的Hidden states 训练 Pretrain，80%情况下随机mask第i个token，然后用cross entropy loss来预测这个token train的时候用了one-hot来标记哪个log_prob是需要用来计算为cross entropy loss的 还有一个任务是Next sentence prediction，计算下一个句子的log_prob 最后两个loss加起来了 Fine-tune: 把transformer的最后一层layer转化成一个适配下游任务的layer，不转不行，所以这个叫Encoder-only的Network。这里是一个classifier的实例，这里把最后一层Hidden states转化成了一个dense layer。 总结一下，Encoder-only的结构必须改模型结构才能使用到下游任务上，必须fine-tune，成本较高。下期讲Encoder-Decoder结构的代表作。]]></summary></entry><entry><title type="html">【AI】SFT，FT，和Multi-task Prompt Tuning还没分清吗</title><link href="http://localhost:4000/ai/ai_algorithms/2024/01/23/llm-sft.html" rel="alternate" type="text/html" title="【AI】SFT，FT，和Multi-task Prompt Tuning还没分清吗" /><published>2024-01-23T19:56:07+08:00</published><updated>2024-01-23T19:56:07+08:00</updated><id>http://localhost:4000/ai/ai_algorithms/2024/01/23/llm-sft</id><content type="html" xml:base="http://localhost:4000/ai/ai_algorithms/2024/01/23/llm-sft.html"><![CDATA[<p>还在纠结这些名词之间的区别吗？给你讲清楚</p>

<p><img src="/assets/images/image-20240129152812980.png" alt="image-20240129152812980" /></p>

<p>TLDR：主要差别在于训练数据的构造。</p>

<ul>
  <li>Pretrain
    <ul>
      <li>无监督的，就纯用语料库来训练，比如webtext，Wikipedia等，预测下一个token的概率分布，并用cross-entropy loss作为loss Function来更新模型的参数；</li>
      <li>Continuous Pretrain：在一个已经训练好的预训练模型上，用一些数据来进一步加强模型的某些方面的能力，这也是无监督的，数据也没有经过特殊构造，就是原始文本输入进去。</li>
    </ul>
  </li>
  <li>Fine-tune：这是比较大的名词，基本上所有在预训练模型上更新参数的方法都可以叫做Fine-tune。与此相关的名词基本上只有数据构造上的区别。
    <ul>
      <li>Supervised Fine-tune：构造了专门的输入输出pair的，基于预训练模型的，都可以叫做SFT。一般来说，如果直接把现有NLP任务的数据不经过改造就直接用于大模型训练，叫做SFT。需要大量数据</li>
      <li>Instruction Tuning：构造了专门的自然语言指令跟随的输入和输出对。例如，原本的情感分析是给定一句话，直接输出高兴，难过，中性等；instruction Tuning是“请告诉这段话蕴含的情感”+输入的那句话：高兴，这样来构造数据。一般需要大量数据。<img src="/assets/images/image-20240129152853378.png" alt="image-20240129152853378" /></li>
      <li>Multi-task Prompt Tuning：把各种任务都构造成自然语言Prompt，只需要很少量数据训练，推理的时候如果和这个Prompt长得像，就可以激发这部分训练数据的能力。<img src="/assets/images/image-20240129152836303.png" alt="image-20240129152836303" /></li>
    </ul>
  </li>
  <li>Alignment Tuning
    <ul>
      <li>RLHF：一般来说需要人工标注Preference，使模型生成的结果对齐人类选择的过程，叫做RLHF。<img src="/assets/images/image-20240129152913271.png" alt="image-20240129152913271" /></li>
    </ul>
  </li>
</ul>]]></content><author><name>Chengru Song</name></author><category term="[&quot;AI&quot;, &quot;AI_Algorithms&quot;]" /><category term="301-work-ai" /><summary type="html"><![CDATA[还在纠结这些名词之间的区别吗？给你讲清楚 TLDR：主要差别在于训练数据的构造。 Pretrain 无监督的，就纯用语料库来训练，比如webtext，Wikipedia等，预测下一个token的概率分布，并用cross-entropy loss作为loss Function来更新模型的参数； Continuous Pretrain：在一个已经训练好的预训练模型上，用一些数据来进一步加强模型的某些方面的能力，这也是无监督的，数据也没有经过特殊构造，就是原始文本输入进去。 Fine-tune：这是比较大的名词，基本上所有在预训练模型上更新参数的方法都可以叫做Fine-tune。与此相关的名词基本上只有数据构造上的区别。 Supervised Fine-tune：构造了专门的输入输出pair的，基于预训练模型的，都可以叫做SFT。一般来说，如果直接把现有NLP任务的数据不经过改造就直接用于大模型训练，叫做SFT。需要大量数据 Instruction Tuning：构造了专门的自然语言指令跟随的输入和输出对。例如，原本的情感分析是给定一句话，直接输出高兴，难过，中性等；instruction Tuning是“请告诉这段话蕴含的情感”+输入的那句话：高兴，这样来构造数据。一般需要大量数据。 Multi-task Prompt Tuning：把各种任务都构造成自然语言Prompt，只需要很少量数据训练，推理的时候如果和这个Prompt长得像，就可以激发这部分训练数据的能力。 Alignment Tuning RLHF：一般来说需要人工标注Preference，使模型生成的结果对齐人类选择的过程，叫做RLHF。]]></summary></entry><entry><title type="html">【AI】Encoder-decoder Transformer</title><link href="http://localhost:4000/ai/ai_algorithms/2024/01/23/llm-encoder-decoder.html" rel="alternate" type="text/html" title="【AI】Encoder-decoder Transformer" /><published>2024-01-23T19:56:07+08:00</published><updated>2024-01-23T19:56:07+08:00</updated><id>http://localhost:4000/ai/ai_algorithms/2024/01/23/llm-encoder-decoder</id><content type="html" xml:base="http://localhost:4000/ai/ai_algorithms/2024/01/23/llm-encoder-decoder.html"><![CDATA[<h1 id="encoder-decoder-transformer">Encoder-Decoder Transformer</h1>

<p>关于Transformer的三种架构的区别，可移步什么是Encoder-only Transformer这篇文章。</p>

<p>本文主要讲Encoder-Decoder Transformer结构，其原始论文是最经典的Attention is all you need.</p>

<ol>
  <li>
    <p>模型介绍</p>

    <ol>
      <li>
        <p>Encoding：文本输入会把文本Encode成一种数字形式，也就是Tokenization，比如asynchronous会被encode成数字28，GPT使用的是BPE encoding方法，这个不赘述，感兴趣的话我可以后面再出一篇文章。</p>
      </li>
      <li>
        <p>b. Encoder-decoder：Input就是src Sequence，中间经过Embedding，Positional Encoding后，形成一个固定长度的表征，再用Decoder对这个表征进行解码</p>

        <ol>
          <li><img src="/assets/images/image-20240129153550213.png" alt="image-20240129153550213" /></li>
        </ol>
      </li>
      <li>
        <p>mask是什么，为什么要用mask？mask实际上是自回归模型的精髓，参考下图，在生成过程中，需要把Decoder全部掩盖起来，每次生成了一个Token后，把当前Token的mask拿掉，这样在下一次生成的时候，Encoder会使用未mask的所有作为Input。</p>

        <ol>
          <li><img src="/assets/images/image-20240129153611656.png" alt="image-20240129153611656" /></li>
        </ol>
      </li>
      <li>
        <p>Mask的大小参考下图，当window的长度为1的时候，模型只能“看见”第一个位置的Input，其他位置的都被mask起来了。</p>

        <ol>
          <li><img src="/assets/images/image-20240129153635305.png" alt="image-20240129153635305" /></li>
        </ol>
      </li>
      <li>
        <p>生成结果：一次生成其实得到的是整个语料库中，当前句子下一个单词的概率分布，我们可以从这个概率分布中sample一个Token出来；</p>

        <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">Generator</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="s">"Define standard linear + softmax generation step."</span>
      
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">vocab</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Generator</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">proj</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">vocab</span><span class="p">)</span>
      
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">log_softmax</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">proj</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
</code></pre></div>        </div>
      </li>
      <li>
        <p>Attention模块，如何理解Attention？其本质是，我想找到<strong>理解一个问题的关键词</strong>。比如“如何理解Encoder-Decoder Transformer？”关键词是Encoder-Decoder Transformer，后面的生成文本就应该主要参考这个关键词给出回答。 图里面的QKV指代的是Query，key，Value，我们把这个想象成一个信息提取系统，先用Q和K的矩阵相乘找到最相关的关键词，再把关键词对应的信息提取出来。</p>

        <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">attention</span><span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="n">mask</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
    <span class="s">"Compute 'Scaled Dot Product Attention'"</span>
    <span class="n">d_k</span> <span class="o">=</span> <span class="n">query</span><span class="p">.</span><span class="n">size</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">scores</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">.</span><span class="n">transpose</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span> <span class="o">/</span> <span class="n">math</span><span class="p">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">d_k</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
        <span class="n">scores</span> <span class="o">=</span> <span class="n">scores</span><span class="p">.</span><span class="n">masked_fill</span><span class="p">(</span><span class="n">mask</span> <span class="o">==</span> <span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mf">1e9</span><span class="p">)</span>
    <span class="n">p_attn</span> <span class="o">=</span> <span class="n">scores</span><span class="p">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">dropout</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
        <span class="n">p_attn</span> <span class="o">=</span> <span class="n">dropout</span><span class="p">(</span><span class="n">p_attn</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">torch</span><span class="p">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">p_attn</span><span class="p">,</span> <span class="n">value</span><span class="p">),</span> <span class="n">p_attn</span>
</code></pre></div>        </div>
      </li>
      <li>
        <p>Embedding，把Input映射到一个Embedding</p>

        <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">Embeddings</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">vocab</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Embeddings</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">lut</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Embedding</span><span class="p">(</span><span class="n">vocab</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">d_model</span> <span class="o">=</span> <span class="n">d_model</span>
      
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="p">.</span><span class="n">lut</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">*</span> <span class="n">math</span><span class="p">.</span><span class="n">sqrt</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">d_model</span><span class="p">)</span>
</code></pre></div>        </div>
      </li>
      <li>
        <p>Positional encoding: 为什么需要加这个，因为纯Embedding是无法感知句子中的单词顺序的，需要在Embedding的基础上，加上一个对Token位置编码后的Vector，具体原理不赘述了。</p>
      </li>
    </ol>
  </li>
</ol>

<p>总结：我们把所有的模块合并起来，就是一个完整的Model</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">make_model</span><span class="p">(</span>
    <span class="n">src_vocab</span><span class="p">,</span> <span class="n">tgt_vocab</span><span class="p">,</span> <span class="n">N</span><span class="o">=</span><span class="mi">6</span><span class="p">,</span> <span class="n">d_model</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span> <span class="n">d_ff</span><span class="o">=</span><span class="mi">2048</span><span class="p">,</span> <span class="n">h</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="mf">0.1</span>
<span class="p">):</span>
    <span class="s">"Helper: Construct a model from hyperparameters."</span>
    <span class="n">c</span> <span class="o">=</span> <span class="n">copy</span><span class="p">.</span><span class="n">deepcopy</span>
    <span class="n">attn</span> <span class="o">=</span> <span class="n">MultiHeadedAttention</span><span class="p">(</span><span class="n">h</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>
    <span class="n">ff</span> <span class="o">=</span> <span class="n">PositionwiseFeedForward</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_ff</span><span class="p">,</span> <span class="n">dropout</span><span class="p">)</span>
    <span class="n">position</span> <span class="o">=</span> <span class="n">PositionalEncoding</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">dropout</span><span class="p">)</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">EncoderDecoder</span><span class="p">(</span>
        <span class="n">Encoder</span><span class="p">(</span><span class="n">EncoderLayer</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">c</span><span class="p">(</span><span class="n">attn</span><span class="p">),</span> <span class="n">c</span><span class="p">(</span><span class="n">ff</span><span class="p">),</span> <span class="n">dropout</span><span class="p">),</span> <span class="n">N</span><span class="p">),</span>
        <span class="n">Decoder</span><span class="p">(</span><span class="n">DecoderLayer</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">c</span><span class="p">(</span><span class="n">attn</span><span class="p">),</span> <span class="n">c</span><span class="p">(</span><span class="n">attn</span><span class="p">),</span> <span class="n">c</span><span class="p">(</span><span class="n">ff</span><span class="p">),</span> <span class="n">dropout</span><span class="p">),</span> <span class="n">N</span><span class="p">),</span>
        <span class="n">nn</span><span class="p">.</span><span class="n">Sequential</span><span class="p">(</span><span class="n">Embeddings</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">src_vocab</span><span class="p">),</span> <span class="n">c</span><span class="p">(</span><span class="n">position</span><span class="p">)),</span>
        <span class="n">nn</span><span class="p">.</span><span class="n">Sequential</span><span class="p">(</span><span class="n">Embeddings</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">tgt_vocab</span><span class="p">),</span> <span class="n">c</span><span class="p">(</span><span class="n">position</span><span class="p">)),</span>
        <span class="n">Generator</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">tgt_vocab</span><span class="p">),</span>
    <span class="p">)</span>

    <span class="c1"># This was important from their code.
</span>    <span class="c1"># Initialize parameters with Glorot / fan_avg.
</span>    <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">model</span><span class="p">.</span><span class="n">parameters</span><span class="p">():</span>
        <span class="k">if</span> <span class="n">p</span><span class="p">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">nn</span><span class="p">.</span><span class="n">init</span><span class="p">.</span><span class="n">xavier_uniform_</span><span class="p">(</span><span class="n">p</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">model</span>
</code></pre></div></div>

<p>下期跟大家讲一下如何对Encoder-Decoder框架的模型训练和评估。感兴趣的小伙伴可以关注一下，我们下期再见。</p>]]></content><author><name>Chengru Song</name></author><category term="[&quot;AI&quot;, &quot;AI_Algorithms&quot;]" /><category term="301-work-ai" /><summary type="html"><![CDATA[Encoder-Decoder Transformer 关于Transformer的三种架构的区别，可移步什么是Encoder-only Transformer这篇文章。 本文主要讲Encoder-Decoder Transformer结构，其原始论文是最经典的Attention is all you need. 模型介绍 Encoding：文本输入会把文本Encode成一种数字形式，也就是Tokenization，比如asynchronous会被encode成数字28，GPT使用的是BPE encoding方法，这个不赘述，感兴趣的话我可以后面再出一篇文章。 b. Encoder-decoder：Input就是src Sequence，中间经过Embedding，Positional Encoding后，形成一个固定长度的表征，再用Decoder对这个表征进行解码 mask是什么，为什么要用mask？mask实际上是自回归模型的精髓，参考下图，在生成过程中，需要把Decoder全部掩盖起来，每次生成了一个Token后，把当前Token的mask拿掉，这样在下一次生成的时候，Encoder会使用未mask的所有作为Input。 Mask的大小参考下图，当window的长度为1的时候，模型只能“看见”第一个位置的Input，其他位置的都被mask起来了。 生成结果：一次生成其实得到的是整个语料库中，当前句子下一个单词的概率分布，我们可以从这个概率分布中sample一个Token出来； class Generator(nn.Module): "Define standard linear + softmax generation step." def __init__(self, d_model, vocab): super(Generator, self).__init__() self.proj = nn.Linear(d_model, vocab) def forward(self, x): return log_softmax(self.proj(x), dim=-1) Attention模块，如何理解Attention？其本质是，我想找到理解一个问题的关键词。比如“如何理解Encoder-Decoder Transformer？”关键词是Encoder-Decoder Transformer，后面的生成文本就应该主要参考这个关键词给出回答。 图里面的QKV指代的是Query，key，Value，我们把这个想象成一个信息提取系统，先用Q和K的矩阵相乘找到最相关的关键词，再把关键词对应的信息提取出来。 def attention(query, key, value, mask=None, dropout=None): "Compute 'Scaled Dot Product Attention'" d_k = query.size(-1) scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k) if mask is not None: scores = scores.masked_fill(mask == 0, -1e9) p_attn = scores.softmax(dim=-1) if dropout is not None: p_attn = dropout(p_attn) return torch.matmul(p_attn, value), p_attn Embedding，把Input映射到一个Embedding class Embeddings(nn.Module): def __init__(self, d_model, vocab): super(Embeddings, self).__init__() self.lut = nn.Embedding(vocab, d_model) self.d_model = d_model def forward(self, x): return self.lut(x) * math.sqrt(self.d_model) Positional encoding: 为什么需要加这个，因为纯Embedding是无法感知句子中的单词顺序的，需要在Embedding的基础上，加上一个对Token位置编码后的Vector，具体原理不赘述了。 总结：我们把所有的模块合并起来，就是一个完整的Model def make_model( src_vocab, tgt_vocab, N=6, d_model=512, d_ff=2048, h=8, dropout=0.1 ): "Helper: Construct a model from hyperparameters." c = copy.deepcopy attn = MultiHeadedAttention(h, d_model) ff = PositionwiseFeedForward(d_model, d_ff, dropout) position = PositionalEncoding(d_model, dropout) model = EncoderDecoder( Encoder(EncoderLayer(d_model, c(attn), c(ff), dropout), N), Decoder(DecoderLayer(d_model, c(attn), c(attn), c(ff), dropout), N), nn.Sequential(Embeddings(d_model, src_vocab), c(position)), nn.Sequential(Embeddings(d_model, tgt_vocab), c(position)), Generator(d_model, tgt_vocab), ) # This was important from their code. # Initialize parameters with Glorot / fan_avg. for p in model.parameters(): if p.dim() &gt; 1: nn.init.xavier_uniform_(p) return model 下期跟大家讲一下如何对Encoder-Decoder框架的模型训练和评估。感兴趣的小伙伴可以关注一下，我们下期再见。]]></summary></entry><entry><title type="html">【AI】元能力探索：监督微调的能力</title><link href="http://localhost:4000/ai/ai_algorithms/2024/01/21/llm-meta-abilities.html" rel="alternate" type="text/html" title="【AI】元能力探索：监督微调的能力" /><published>2024-01-21T20:56:07+08:00</published><updated>2024-01-21T20:56:07+08:00</updated><id>http://localhost:4000/ai/ai_algorithms/2024/01/21/llm-meta-abilities</id><content type="html" xml:base="http://localhost:4000/ai/ai_algorithms/2024/01/21/llm-meta-abilities.html"><![CDATA[<p>大模型时代，数据质量远远大于数据质量。</p>

<p>在解决业务问题时，我们经常需要把大模型适用于各种不同的下游任务。此时一个常用的方法是有监督微调。例如情感识别，输入是，我今天被老师表扬了，输出是，高兴。我们可以构造这样输入输出对，就可以对大模型有监督微调。</p>

<p>那么，多少数据能够让模型以较高的准确率完成指令跟随呢？答案是仅需几百条。</p>

<p>在我们的场景下，我们仅筛选了400条高质量数据，对模型进行有监督微调，就能让这个模型的以较高的准确率完成该任务，更令人震惊的是，其中仅有24条包含某指令，而未来有类似指令的时候，模型也能准确识别到该指令的输入而产生对应的输出。</p>

<p>总结一下，当我们需求仅是指令跟随能力的较为简单的任务时，使用高质量数据远好于多个低质量数据。</p>]]></content><author><name>Chengru Song</name></author><category term="[&quot;AI&quot;, &quot;AI_Algorithms&quot;]" /><category term="301-work-ai" /><summary type="html"><![CDATA[大模型时代，数据质量远远大于数据质量。 在解决业务问题时，我们经常需要把大模型适用于各种不同的下游任务。此时一个常用的方法是有监督微调。例如情感识别，输入是，我今天被老师表扬了，输出是，高兴。我们可以构造这样输入输出对，就可以对大模型有监督微调。 那么，多少数据能够让模型以较高的准确率完成指令跟随呢？答案是仅需几百条。 在我们的场景下，我们仅筛选了400条高质量数据，对模型进行有监督微调，就能让这个模型的以较高的准确率完成该任务，更令人震惊的是，其中仅有24条包含某指令，而未来有类似指令的时候，模型也能准确识别到该指令的输入而产生对应的输出。 总结一下，当我们需求仅是指令跟随能力的较为简单的任务时，使用高质量数据远好于多个低质量数据。]]></summary></entry><entry><title type="html">【AI】大模型算法如何避免成为Prompt Engineer和数据清洗师</title><link href="http://localhost:4000/ai/ai_basics/2024/01/15/prompting-avoidance.html" rel="alternate" type="text/html" title="【AI】大模型算法如何避免成为Prompt Engineer和数据清洗师" /><published>2024-01-15T08:56:07+08:00</published><updated>2024-01-15T08:56:07+08:00</updated><id>http://localhost:4000/ai/ai_basics/2024/01/15/prompting-avoidance</id><content type="html" xml:base="http://localhost:4000/ai/ai_basics/2024/01/15/prompting-avoidance.html"><![CDATA[<ol>
  <li><strong>把prompting交给他人</strong>：Prompting是提升效果的重要手段，这个步骤是必要的，可以把这步交给产品或者后端，你只用效果最好的Prompt；</li>
  <li><strong>只做一次数据清洗</strong>：数据对大模型效果有<strong>至关重要</strong>的作用，这步必须自己做，但这个过程可以结合自己对业务的理解做一次非常全的数据清洗，然后训练一版模型，跑一版benchmark，如果各个指标都有提升，剩下的交给产品或者后端做基于新训练的模型做prompting，自己再同步做别的事情；</li>
  <li><strong>理解模型元能力提升的关键</strong>：大模型的指令跟随能力，推理能力，上下文学习能力，都是模型的元能力，这些能力是由一些特殊的训练方法和数据习得的，掌握了提升模型元能力的方法，就能对整体效果的把控性更强，也能进阶成更好的大模型算法。可以在第二步中多多探索不同数据对于模型各项能力benchmark的提升；</li>
  <li><strong>合并迭代</strong>：效果的优化往往需要较长时间，学会拒绝不能完成的效果提升或者合并几次提升到一次迭代里，否则会陷入到做业务需求的死循环中。</li>
</ol>]]></content><author><name>Chengru Song</name></author><category term="[&quot;AI&quot;, &quot;AI_Basics&quot;]" /><category term="301-work-ai" /><summary type="html"><![CDATA[把prompting交给他人：Prompting是提升效果的重要手段，这个步骤是必要的，可以把这步交给产品或者后端，你只用效果最好的Prompt； 只做一次数据清洗：数据对大模型效果有至关重要的作用，这步必须自己做，但这个过程可以结合自己对业务的理解做一次非常全的数据清洗，然后训练一版模型，跑一版benchmark，如果各个指标都有提升，剩下的交给产品或者后端做基于新训练的模型做prompting，自己再同步做别的事情； 理解模型元能力提升的关键：大模型的指令跟随能力，推理能力，上下文学习能力，都是模型的元能力，这些能力是由一些特殊的训练方法和数据习得的，掌握了提升模型元能力的方法，就能对整体效果的把控性更强，也能进阶成更好的大模型算法。可以在第二步中多多探索不同数据对于模型各项能力benchmark的提升； 合并迭代：效果的优化往往需要较长时间，学会拒绝不能完成的效果提升或者合并几次提升到一次迭代里，否则会陷入到做业务需求的死循环中。]]></summary></entry><entry><title type="html">【AI】Scaling multimodal understanding to long videos</title><link href="http://localhost:4000/ai/ai_basics/2023/11/24/video-modal.html" rel="alternate" type="text/html" title="【AI】Scaling multimodal understanding to long videos" /><published>2023-11-24T08:56:07+08:00</published><updated>2023-11-24T08:56:07+08:00</updated><id>http://localhost:4000/ai/ai_basics/2023/11/24/video-modal</id><content type="html" xml:base="http://localhost:4000/ai/ai_basics/2023/11/24/video-modal.html"><![CDATA[<p>https://arxiv.org/pdf/2311.05698.pdf</p>

<p>AJ Piergiovanni，Google Research</p>

<h1 id="场景与问题">场景与问题</h1>

<p>解决不同模态之间的Heterogenious Input问题是一个重要课题。因为</p>

<ol>
  <li>输入体积上，视频和音频在体积上比文字大得多，因此这两者Input信息量是无法对齐的，需要通过模型对齐。</li>
  <li>数据处理上，instruction following的Video QA模型的训练数据中包含提取出的文本信息（标题，简介等）是全局信息，而视频和音频都是和时间对齐的，其本身没有全局属性。</li>
</ol>

<h1 id="一般思路">一般思路</h1>

<ol>
  <li>Tokenize visual input - LLaVA</li>
</ol>]]></content><author><name>Chengru Song</name></author><category term="[&quot;AI&quot;, &quot;AI_Basics&quot;]" /><category term="301-work-ai" /><summary type="html"><![CDATA[https://arxiv.org/pdf/2311.05698.pdf AJ Piergiovanni，Google Research 场景与问题 解决不同模态之间的Heterogenious Input问题是一个重要课题。因为 输入体积上，视频和音频在体积上比文字大得多，因此这两者Input信息量是无法对齐的，需要通过模型对齐。 数据处理上，instruction following的Video QA模型的训练数据中包含提取出的文本信息（标题，简介等）是全局信息，而视频和音频都是和时间对齐的，其本身没有全局属性。 一般思路 Tokenize visual input - LLaVA]]></summary></entry><entry><title type="html">【AI】LLM Learning</title><link href="http://localhost:4000/ai/ai_basics/2023/11/03/llm-learning.html" rel="alternate" type="text/html" title="【AI】LLM Learning" /><published>2023-11-03T22:56:07+08:00</published><updated>2023-11-03T22:56:07+08:00</updated><id>http://localhost:4000/ai/ai_basics/2023/11/03/llm-learning</id><content type="html" xml:base="http://localhost:4000/ai/ai_basics/2023/11/03/llm-learning.html"><![CDATA[<h1 id="pretrain">Pretrain</h1>

<h2 id="performance-vs-data--size">Performance v.s. Data &amp; Size</h2>

<blockquote>
  <p>For a given compute budget, the best performances are not achieved by the largest models, but by smaller models trained on more data. – from LLaMA</p>
</blockquote>

<p>用更多的数据训练，Size小一点也会有更好的效果。</p>

<p>LLaMA</p>

<p>Encoding: BPE</p>

<p>Training Data: 1.4T token, Wikipedia和Books Domain训练了两个epochs</p>

<p>Epoch meaning:</p>

<blockquote>
  <p><a href="https://deepai.org/machine-learning-glossary-and-terms/epoch">In the context of machine learning, an epoch is one complete pass through the training data</a><a href="https://deepai.org/machine-learning-glossary-and-terms/epoch">1</a>. It is typical to train a deep neural network for multiple epochs, meaning that the same data is used repeatedly to update the model’s parameters.</p>
</blockquote>]]></content><author><name>Chengru Song</name></author><category term="[&quot;AI&quot;, &quot;AI_Basics&quot;]" /><category term="301-work-ai" /><summary type="html"><![CDATA[Pretrain Performance v.s. Data &amp; Size For a given compute budget, the best performances are not achieved by the largest models, but by smaller models trained on more data. – from LLaMA 用更多的数据训练，Size小一点也会有更好的效果。 LLaMA Encoding: BPE Training Data: 1.4T token, Wikipedia和Books Domain训练了两个epochs Epoch meaning: In the context of machine learning, an epoch is one complete pass through the training data1. It is typical to train a deep neural network for multiple epochs, meaning that the same data is used repeatedly to update the model’s parameters.]]></summary></entry><entry><title type="html">【AI】LLM Prompting</title><link href="http://localhost:4000/ai/ai_basics/2023/10/28/llm-prompting.html" rel="alternate" type="text/html" title="【AI】LLM Prompting" /><published>2023-10-28T16:20:07+08:00</published><updated>2023-10-28T16:20:07+08:00</updated><id>http://localhost:4000/ai/ai_basics/2023/10/28/llm-prompting</id><content type="html" xml:base="http://localhost:4000/ai/ai_basics/2023/10/28/llm-prompting.html"><![CDATA[<h1 id="background">Background</h1>

<p>要解决AIGC业务落地的问题，在做特别hardcore的事情之前，至少有三个方向可以考虑。</p>

<ol>
  <li>Prompting，直接给LLM写好prompt，通过few-shots，CoT等技巧，直接让GPT生成结果。</li>
  <li>Agent，设定一个目标，让GPT通过CoT生成Task，解决Task等方法最终直接解决问题。</li>
  <li>SFT，直接在产出结果后面加一个layer，fine-tune一下，加上1和2的一些方法，能否达到预期的效果。</li>
</ol>

<h1 id="prompt-engineering">Prompt Engineering</h1>

<h2 id="roadmap">Roadmap</h2>

<p><a href="https://roadmap.sh/prompt-engineering">Roadmap</a></p>

<p><img src="/assets/images/image-20231028162648178.png" alt="image-20231028162648178" /></p>

<p><img src="/assets/images/image-20231106140554116.png" alt="image-20231106140554116" /></p>]]></content><author><name>Chengru Song</name></author><category term="[&quot;AI&quot;, &quot;AI_Basics&quot;]" /><category term="301-work-ai" /><summary type="html"><![CDATA[Background 要解决AIGC业务落地的问题，在做特别hardcore的事情之前，至少有三个方向可以考虑。 Prompting，直接给LLM写好prompt，通过few-shots，CoT等技巧，直接让GPT生成结果。 Agent，设定一个目标，让GPT通过CoT生成Task，解决Task等方法最终直接解决问题。 SFT，直接在产出结果后面加一个layer，fine-tune一下，加上1和2的一些方法，能否达到预期的效果。 Prompt Engineering Roadmap Roadmap]]></summary></entry><entry><title type="html">【AI】LLM Agents</title><link href="http://localhost:4000/ai/ai_basics/2023/10/28/llm-agents.html" rel="alternate" type="text/html" title="【AI】LLM Agents" /><published>2023-10-28T14:30:07+08:00</published><updated>2023-10-28T14:30:07+08:00</updated><id>http://localhost:4000/ai/ai_basics/2023/10/28/llm-agents</id><content type="html" xml:base="http://localhost:4000/ai/ai_basics/2023/10/28/llm-agents.html"><![CDATA[<h1 id="reference">Reference</h1>

<ol>
  <li><a href="https://github.com/xlang-ai/OpenAgents">xlang-ai/OpenAgents: OpenAgents: An Open Platform for Language Agents in the Wild (github.com)</a></li>
</ol>

<h1 id="definition">Definition</h1>

<ol>
  <li>Agent
    <ol>
      <li>设定目标</li>
      <li>breakdown the goal step by step</li>
      <li>setup tasks for the goal</li>
      <li>produce results for the goal</li>
    </ol>
  </li>
  <li>OpenAI plugin
    <ol>
      <li>Interact with external entities to accomplish a specific task.</li>
    </ol>
  </li>
</ol>]]></content><author><name>Chengru Song</name></author><category term="[&quot;AI&quot;, &quot;AI_Basics&quot;]" /><category term="301-work-ai" /><summary type="html"><![CDATA[Reference xlang-ai/OpenAgents: OpenAgents: An Open Platform for Language Agents in the Wild (github.com) Definition Agent 设定目标 breakdown the goal step by step setup tasks for the goal produce results for the goal OpenAI plugin Interact with external entities to accomplish a specific task.]]></summary></entry><entry><title type="html">【Blog】First Principle</title><link href="http://localhost:4000/work/blog/2023/10/16/first-principle.html" rel="alternate" type="text/html" title="【Blog】First Principle" /><published>2023-10-16T15:27:07+08:00</published><updated>2023-10-16T15:27:07+08:00</updated><id>http://localhost:4000/work/blog/2023/10/16/first-principle</id><content type="html" xml:base="http://localhost:4000/work/blog/2023/10/16/first-principle.html"><![CDATA[<h1 id="reference">Reference</h1>

<ol>
  <li><a href="https://medium.com/@idtimw/思维模型03-first-principles-第一性原理-7571fc664faf">思维模型03 — First Principles  第一性原理  by ID.TIMW  Medium</a></li>
</ol>

<h1 id="what">What</h1>

<p>第一原理（英语：First principle），哲学与逻辑名词，是一个最基本的命题或假设，不能被省略或删除，也不能被违反。第一原理相当于是在数学中的公理。最早由亚里斯多德提出。</p>

<h2 id="苏格拉底式提问">苏格拉底式提问</h2>

<p>对问题使用苏格拉底式提问是一个锻炼第一性原理思考的方法。</p>

<ol>
  <li>理清思维，寻找问题的源头：问题出在哪里，具体表现形式有哪些？</li>
  <li>挑战假设：这个情况总是发生么？什么因素会导致问题出现？</li>
  <li>证据为基础的论点：假设的证据在哪里？整理从哪里来的？证据是否可靠？</li>
  <li>替代观点和角度/冲击其它想法：有什么其他的观点可以反驳？有没有其它的方法可以解决。</li>
  <li>影响和后果：如果使用了方法会导致什么后果？</li>
  <li>质疑问题：为什么会有这种疑问？为什么这个问题很重要？你认为哪个问题最有用？</li>
</ol>]]></content><author><name>Chengru Song</name></author><category term="[&quot;work&quot;, &quot;Blog&quot;]" /><category term="301-work-blog" /><summary type="html"><![CDATA[Reference 思维模型03 — First Principles 第一性原理 by ID.TIMW Medium What 第一原理（英语：First principle），哲学与逻辑名词，是一个最基本的命题或假设，不能被省略或删除，也不能被违反。第一原理相当于是在数学中的公理。最早由亚里斯多德提出。 苏格拉底式提问 对问题使用苏格拉底式提问是一个锻炼第一性原理思考的方法。 理清思维，寻找问题的源头：问题出在哪里，具体表现形式有哪些？ 挑战假设：这个情况总是发生么？什么因素会导致问题出现？ 证据为基础的论点：假设的证据在哪里？整理从哪里来的？证据是否可靠？ 替代观点和角度/冲击其它想法：有什么其他的观点可以反驳？有没有其它的方法可以解决。 影响和后果：如果使用了方法会导致什么后果？ 质疑问题：为什么会有这种疑问？为什么这个问题很重要？你认为哪个问题最有用？]]></summary></entry></feed>