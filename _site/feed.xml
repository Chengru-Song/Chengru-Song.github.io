<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="zh-CN"><generator uri="https://jekyllrb.com/" version="4.2.1">Jekyll</generator><link href="http://0.0.0.0:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://0.0.0.0:4000/" rel="alternate" type="text/html" hreflang="zh-CN" /><updated>2023-06-18T09:26:59+08:00</updated><id>http://0.0.0.0:4000/feed.xml</id><title type="html">Chengru’s Blog</title><subtitle>A personal blog website for sharing of technology, reflection and branding. 
</subtitle><author><name>Chengru Song</name></author><entry><title type="html">【AI】LLM Deep Dive</title><link href="http://0.0.0.0:4000/ai/ai_basics/2023/06/08/LLM-deep-dive.html" rel="alternate" type="text/html" title="【AI】LLM Deep Dive" /><published>2023-06-08T07:46:07+08:00</published><updated>2023-06-08T07:46:07+08:00</updated><id>http://0.0.0.0:4000/ai/ai_basics/2023/06/08/LLM-deep-dive</id><content type="html" xml:base="http://0.0.0.0:4000/ai/ai_basics/2023/06/08/LLM-deep-dive.html"><![CDATA[<h1 id="overview">Overview</h1>

<ol>
  <li>什么是LLM？</li>
  <li>LLM的Intuitive是什么？</li>
  <li>LLM的原理是什么，底层是如何实现的？</li>
  <li>相比于其他方法，LLM为什么能够达到更好的效果？</li>
  <li>LLM产业运行的难点在哪里？</li>
  <li>如果我现在起步，做和LLM什么相关工作比较好？机会点在哪里？</li>
  <li>如果有一个LLM相关工作的Roadmap，这个Roadmap是什么？</li>
  <li>如何与我现在的工作内容产生联系，让我更好起步？</li>
</ol>

<p>因为我算是LLM领域的小白，所以我想从NLP的历史出发，看看如何一步步演变成目前的形态。</p>

<h1 id="background">Background</h1>

<p>NLP的Intuitive是什么？为什么这种方法可行。</p>

<h2 id="intuitive">Intuitive</h2>

<h3 id="statistical-model">Statistical Model</h3>

<p>参考这篇2001年的论文<a href="https://arxiv.org/pdf/cs/0108005.pdf">A Bit of Progress in Language Modeling</a>， 统计模型主要解决的问题是，<strong>一个单词序列出现的概率有多大</strong>。即给定一个单词序列$\omega_1, \omega_2, …, \omega_n$ ，这组单词序列出现的概率定义为$P(\omega_1…\omega_n)$，这个概率key被定义为一个组合概率：</p>

\[P(\omega_1...\omega_n) = P(\omega_1) \times P(\omega_2 | \omega_1) \times P(\omega_3|\omega_1\omega_2) \times...\times P(\omega_n|\omega_1...\omega_{n-1})\]

<p>但是句子的长度是不固定的，对于任意长度的句子，如果都用这样的概率计算，那么这个计算量是非常大的，因此有n-gram的概念，即在某$n-1$个单词出现后，第$n$个是某个单词的概率是多大。</p>

<p>前两个词已经出现的情况下，第三个词出现的条件概率可以简化为在一个语料库中出现的频率。</p>

\[P(\omega_i|\omega_{i-1}\omega_{i-2}) \approx \frac{C(\omega_i)C(\omega_{i-1})C(\omega_{i-2})}{C(\omega_{i-1})C(\omega_{i-2})}\]

<p>这些概率模型有一些问题，比如光看统计特征，对于经常出现的或者只出现一次的词会有异常，因此引入了smoothing方法，这里不再赘述。</p>

<p>这些方法都有一个本质的问题，就是扩展这个序列的长度就能收获更好的效果，但同时扩展这个长度会增加计算量。因此如何获取到词与词之间的关系，尤其是上下文比较长的时候，对Statistic  Model来说是一个无法解决的问题。简称<strong>the curse of dimensionality</strong>。</p>

<h3 id="neural-statistic-network-model">Neural Statistic Network Model</h3>

<p>NLP统计学模型的dimensionality的本质是什么？假如语料库中有$n$个单词，其本质就是这$n$个单词的联立概率分布。即给定$i-1$个单词，模型可以找到下一个概率最大的单词。考虑到这个概率模型过于庞大，因此在2003年的文章<a href="https://jmlr.csail.mit.edu/papers/volume3/bengio03a/bengio03a.pdf">A Neural Probabilistic Language Model</a>中提到可以使用神经网络将单词转化为一个特征向量，并用这个特征向量计算单词之间的概率分布。本质上是用神经网络来拟合这个联立概率方程。优点是这个特征向量是一个高密度向量，易于计算。</p>

<p>这个模型的输入是上下文转化后的dense feature vector，输出是和这个单词相关的下个词的概率。即，</p>

\[f(\omega_t,..., \omega_{t-n+1}) = \hat{P}(\omega_t | \omega_{t-1}...\omega_{t-n+1})\]

<p>因此在训练时，设置损失函数是输出函数的log likelihood，即最大化以下这个方程时的$\theta$.</p>

\[L = \frac{1}{T}\sum_{t}\log f(\omega_t,\omega_{t-1},...,\omega_{t-n+1};\theta) + R(\theta)\]

<p>神经网络结构如下图所示：</p>

<p><img src="/assets/images/image-20230616221548494.png" alt="image-20230616221548494" /></p>]]></content><author><name>Chengru Song</name></author><category term="[&quot;AI&quot;, &quot;AI_Basics&quot;]" /><category term="301-work-ai" /><summary type="html"><![CDATA[Overview 什么是LLM？ LLM的Intuitive是什么？ LLM的原理是什么，底层是如何实现的？ 相比于其他方法，LLM为什么能够达到更好的效果？ LLM产业运行的难点在哪里？ 如果我现在起步，做和LLM什么相关工作比较好？机会点在哪里？ 如果有一个LLM相关工作的Roadmap，这个Roadmap是什么？ 如何与我现在的工作内容产生联系，让我更好起步？ 因为我算是LLM领域的小白，所以我想从NLP的历史出发，看看如何一步步演变成目前的形态。 Background NLP的Intuitive是什么？为什么这种方法可行。 Intuitive Statistical Model 参考这篇2001年的论文A Bit of Progress in Language Modeling， 统计模型主要解决的问题是，一个单词序列出现的概率有多大。即给定一个单词序列$\omega_1, \omega_2, …, \omega_n$ ，这组单词序列出现的概率定义为$P(\omega_1…\omega_n)$，这个概率key被定义为一个组合概率： \[P(\omega_1...\omega_n) = P(\omega_1) \times P(\omega_2 | \omega_1) \times P(\omega_3|\omega_1\omega_2) \times...\times P(\omega_n|\omega_1...\omega_{n-1})\] 但是句子的长度是不固定的，对于任意长度的句子，如果都用这样的概率计算，那么这个计算量是非常大的，因此有n-gram的概念，即在某$n-1$个单词出现后，第$n$个是某个单词的概率是多大。 前两个词已经出现的情况下，第三个词出现的条件概率可以简化为在一个语料库中出现的频率。 \[P(\omega_i|\omega_{i-1}\omega_{i-2}) \approx \frac{C(\omega_i)C(\omega_{i-1})C(\omega_{i-2})}{C(\omega_{i-1})C(\omega_{i-2})}\] 这些概率模型有一些问题，比如光看统计特征，对于经常出现的或者只出现一次的词会有异常，因此引入了smoothing方法，这里不再赘述。 这些方法都有一个本质的问题，就是扩展这个序列的长度就能收获更好的效果，但同时扩展这个长度会增加计算量。因此如何获取到词与词之间的关系，尤其是上下文比较长的时候，对Statistic Model来说是一个无法解决的问题。简称the curse of dimensionality。 Neural Statistic Network Model NLP统计学模型的dimensionality的本质是什么？假如语料库中有$n$个单词，其本质就是这$n$个单词的联立概率分布。即给定$i-1$个单词，模型可以找到下一个概率最大的单词。考虑到这个概率模型过于庞大，因此在2003年的文章A Neural Probabilistic Language Model中提到可以使用神经网络将单词转化为一个特征向量，并用这个特征向量计算单词之间的概率分布。本质上是用神经网络来拟合这个联立概率方程。优点是这个特征向量是一个高密度向量，易于计算。 这个模型的输入是上下文转化后的dense feature vector，输出是和这个单词相关的下个词的概率。即， \[f(\omega_t,..., \omega_{t-n+1}) = \hat{P}(\omega_t | \omega_{t-1}...\omega_{t-n+1})\] 因此在训练时，设置损失函数是输出函数的log likelihood，即最大化以下这个方程时的$\theta$. \[L = \frac{1}{T}\sum_{t}\log f(\omega_t,\omega_{t-1},...,\omega_{t-n+1};\theta) + R(\theta)\] 神经网络结构如下图所示：]]></summary></entry><entry><title type="html">【AI】06/07 Github Trending</title><link href="http://0.0.0.0:4000/ai/ai_basics/2023/06/08/0607github-trending.html" rel="alternate" type="text/html" title="【AI】06/07 Github Trending" /><published>2023-06-08T07:46:07+08:00</published><updated>2023-06-08T07:46:07+08:00</updated><id>http://0.0.0.0:4000/ai/ai_basics/2023/06/08/0607github-trending</id><content type="html" xml:base="http://0.0.0.0:4000/ai/ai_basics/2023/06/08/0607github-trending.html"><![CDATA[<h1 id="top-5-summary">Top-5 Summary</h1>

<ol>
  <li><a href="https://github.com/TransformerOptimus">TransformerOptimus</a>/<strong><a href="https://github.com/TransformerOptimus/SuperAGI">SuperAGI</a></strong>
    <ol>
      <li>SuperAGI是一个用于构建和运行有用的自主代理的框架。SuperAGI旨在构建基础设施，以实现这一目标。使用SuperAGI，您可以提供、生成和部署有用的自主人工智能代理。简单来说，SuperAGI是一套带UI界面的解决方案，你可以构建自己的数据库，embedding等，通过OpenAI的接口来和数据库的内容交互。</li>
    </ol>
  </li>
  <li><a href="https://github.com/ruanyf">ruanyf</a>/<strong><a href="https://github.com/ruanyf/weekly">weekly</a></strong>
    <ol>
      <li>一个科技爱好者周刊</li>
    </ol>
  </li>
  <li><a href="https://github.com/mlc-ai">mlc-ai</a>/<strong><a href="https://github.com/mlc-ai/mlc-llm">mlc-llm</a></strong>
    <ol>
      <li>MLC LLM（多语言模型转换与优化引擎）是一个通用解决方案，可以使任何语言模型在各种硬件后端和本地应用程序上原生部署，并提供一个高效的框架，让每个人都可以进一步优化模型性能以适应自己的用例。可以理解为一个<strong>LLM Edge Computing</strong>套件，甚至还有WebLLM。</li>
    </ol>
  </li>
  <li><a href="https://github.com/SysCV">SysCV</a>/<strong><a href="https://github.com/SysCV/sam-hq">sam-hq</a></strong>
    <ol>
      <li><a href="https://arxiv.org/abs/2306.01567"><strong>Segment Anything in High Quality</strong></a> 这篇CV paper的开源代码，主要做CV中的segmentation，可以segment图像中的所有物体。</li>
    </ol>
  </li>
  <li><a href="https://github.com/hemansnation">hemansnation</a>/<strong><a href="https://github.com/hemansnation/God-Level-Data-Science-ML-Full-Stack">God-Level-Data-Science-ML-Full-Stack</a></strong>
    <ol>
      <li>光看标题就知道了，ML全栈工程师的学习课程。</li>
    </ol>
  </li>
</ol>]]></content><author><name>Chengru Song</name></author><category term="[&quot;AI&quot;, &quot;AI_Basics&quot;]" /><category term="301-work-ai" /><summary type="html"><![CDATA[Top-5 Summary TransformerOptimus/SuperAGI SuperAGI是一个用于构建和运行有用的自主代理的框架。SuperAGI旨在构建基础设施，以实现这一目标。使用SuperAGI，您可以提供、生成和部署有用的自主人工智能代理。简单来说，SuperAGI是一套带UI界面的解决方案，你可以构建自己的数据库，embedding等，通过OpenAI的接口来和数据库的内容交互。 ruanyf/weekly 一个科技爱好者周刊 mlc-ai/mlc-llm MLC LLM（多语言模型转换与优化引擎）是一个通用解决方案，可以使任何语言模型在各种硬件后端和本地应用程序上原生部署，并提供一个高效的框架，让每个人都可以进一步优化模型性能以适应自己的用例。可以理解为一个LLM Edge Computing套件，甚至还有WebLLM。 SysCV/sam-hq Segment Anything in High Quality 这篇CV paper的开源代码，主要做CV中的segmentation，可以segment图像中的所有物体。 hemansnation/God-Level-Data-Science-ML-Full-Stack 光看标题就知道了，ML全栈工程师的学习课程。]]></summary></entry><entry><title type="html">Ai Home</title><link href="http://0.0.0.0:4000/2023/05/10/ai-home.html" rel="alternate" type="text/html" title="Ai Home" /><published>2023-05-10T00:00:00+08:00</published><updated>2023-05-10T00:00:00+08:00</updated><id>http://0.0.0.0:4000/2023/05/10/ai-home</id><content type="html" xml:base="http://0.0.0.0:4000/2023/05/10/ai-home.html"><![CDATA[<ul>
  
      
      
      08/06/2023
      <font size="3"> &lt;a href=/ai/ai_basics/2023/06/08/LLM-deep-dive.html title=AI&gt; 【AI】LLM Deep Dive &lt;/a&gt; </font>
      <br />
      
      08/06/2023
      <font size="3"> &lt;a href=/ai/ai_basics/2023/06/08/0607github-trending.html title=AI&gt; 【AI】06/07 Github Trending &lt;/a&gt; </font>
      <br />
      
      08/05/2023
      <font size="3"> &lt;a href=/ai/ai_basics/2023/05/08/llm-thought.html title=AI&gt; 【AI】LLM Thoughts &lt;/a&gt; </font>
      <br />
      
      25/02/2022
      <font size="3"> &lt;a href=/ai/ai_basics/2022/02/25/ml-basic-knowledge.html title=AI&gt; 【AI】ML Basic Knowledge &lt;/a&gt; </font>
      <br />
      
      07/12/2021
      <font size="3"> &lt;a href=/ai/ai_basics/2021/12/07/item2vec.html title=AI&gt; 【AI】Item2Vec详解 &lt;/a&gt; </font>
      <br />
      
      07/12/2021
      <font size="3"> &lt;a href=/ai/ai_basics/2021/12/07/WAMPE.html title=AI&gt; 【AI】WMAPE详解 &lt;/a&gt; </font>
      <br />
      
      07/12/2021
      <font size="3"> &lt;a href=/ai/ai_algorithms/2021/12/07/LSTM-CNN.html title=AI&gt; 【AI】LSTM+CNN详解 &lt;/a&gt; </font>
      <br />
      
      06/12/2021
      <font size="3"> &lt;a href=/ai/ai_basics/2021/12/06/one-hot.html title=AI&gt; 【AI】One-hot Vector实例 &lt;/a&gt; </font>
      <br />
      
      06/12/2021
      <font size="3"> &lt;a href=/ai/ai_algorithms/2021/12/06/esmm.html title=AI&gt; 【AI】ESMM CVR预测详解 &lt;/a&gt; </font>
      <br />
      
      05/12/2021
      <font size="3"> &lt;a href=/ai/ai_algorithms/2021/12/05/wide_&amp;_deep.html title=AI&gt; 【AI】Wide&amp;Deep Item-based content retrieval(基于内容的协同过滤) &lt;/a&gt; </font>
      <br />
      
      05/12/2021
      <font size="3"> &lt;a href=/ai/ai_basics/2021/12/05/logistic-regression.html title=AI&gt; 【AI】Logistic Regression详解 &lt;/a&gt; </font>
      <br />
      
      05/12/2021
      <font size="3"> &lt;a href=/ai/ai_algorithms/2021/12/05/dsmm.html title=AI&gt; 【AI】DSMM Item-based系统过滤 &lt;/a&gt; </font>
      <br />
      
      05/12/2021
      <font size="3"> &lt;a href=/ai/ai_basics/2021/12/05/cross-entropy-loss.html title=AI&gt; 【AI】Cross Entropy Loss详解 &lt;/a&gt; </font>
      <br />
      
      01/12/2021
      <font size="3"> &lt;a href=/ai/ai_basics/2021/12/01/unigram-distribution.html title=AI&gt; 【AI】Unigram Distribution(NLP中常用) &lt;/a&gt; </font>
      <br />
      
      01/12/2021
      <font size="3"> &lt;a href=/ai/ai_algorithms/2021/12/01/gbdt.html title=AI&gt; 【AI】GBDT(Gradient Boosting Decision Tree)详解 &lt;/a&gt; </font>
      <br />
      
      25/11/2021
      <font size="3"> &lt;a href=/ai/ai_algorithms/2021/11/25/ARIMA.html title=AI&gt; 【AI】Arima模型详解 &lt;/a&gt; </font>
      <br />
      
      21/11/2021
      <font size="3"> &lt;a href=/ai/ai_basics/2021/11/21/term-vector.html title=AI&gt; 【AI】Term-Vector含义 &lt;/a&gt; </font>
      <br />
      
      20/11/2021
      <font size="3"> &lt;a href=/ai/ai_basics/2021/11/20/softmax.html title=AI&gt; 【AI】Softmax函数 &lt;/a&gt; </font>
      <br />
      
      19/11/2021
      <font size="3"> &lt;a href=/ai/ads/2021/11/19/Recommend.html title=AI&gt; 【Ad】Recommendation System Overview(推荐系统详解) &lt;/a&gt; </font>
      <br />
      
      
  
      
  
      
  
      
  
      
  
      
  
      
  
      
  
      
  
      
  
      
  
      
  
      
  
      
  
      
  
      
  
      
  
      
  
      
  
      
  
      
  
      
  
      
  
      
  
      
  
      
  
</ul>]]></content><author><name>Chengru Song</name></author><summary type="html"><![CDATA[08/06/2023 &lt;a href=/ai/ai_basics/2023/06/08/LLM-deep-dive.html title=AI&gt; 【AI】LLM Deep Dive &lt;/a&gt; 08/06/2023 &lt;a href=/ai/ai_basics/2023/06/08/0607github-trending.html title=AI&gt; 【AI】06/07 Github Trending &lt;/a&gt; 08/05/2023 &lt;a href=/ai/ai_basics/2023/05/08/llm-thought.html title=AI&gt; 【AI】LLM Thoughts &lt;/a&gt; 25/02/2022 &lt;a href=/ai/ai_basics/2022/02/25/ml-basic-knowledge.html title=AI&gt; 【AI】ML Basic Knowledge &lt;/a&gt; 07/12/2021 &lt;a href=/ai/ai_basics/2021/12/07/item2vec.html title=AI&gt; 【AI】Item2Vec详解 &lt;/a&gt; 07/12/2021 &lt;a href=/ai/ai_basics/2021/12/07/WAMPE.html title=AI&gt; 【AI】WMAPE详解 &lt;/a&gt; 07/12/2021 &lt;a href=/ai/ai_algorithms/2021/12/07/LSTM-CNN.html title=AI&gt; 【AI】LSTM+CNN详解 &lt;/a&gt; 06/12/2021 &lt;a href=/ai/ai_basics/2021/12/06/one-hot.html title=AI&gt; 【AI】One-hot Vector实例 &lt;/a&gt; 06/12/2021 &lt;a href=/ai/ai_algorithms/2021/12/06/esmm.html title=AI&gt; 【AI】ESMM CVR预测详解 &lt;/a&gt; 05/12/2021 &lt;a href=/ai/ai_algorithms/2021/12/05/wide_&amp;_deep.html title=AI&gt; 【AI】Wide&amp;Deep Item-based content retrieval(基于内容的协同过滤) &lt;/a&gt; 05/12/2021 &lt;a href=/ai/ai_basics/2021/12/05/logistic-regression.html title=AI&gt; 【AI】Logistic Regression详解 &lt;/a&gt; 05/12/2021 &lt;a href=/ai/ai_algorithms/2021/12/05/dsmm.html title=AI&gt; 【AI】DSMM Item-based系统过滤 &lt;/a&gt; 05/12/2021 &lt;a href=/ai/ai_basics/2021/12/05/cross-entropy-loss.html title=AI&gt; 【AI】Cross Entropy Loss详解 &lt;/a&gt; 01/12/2021 &lt;a href=/ai/ai_basics/2021/12/01/unigram-distribution.html title=AI&gt; 【AI】Unigram Distribution(NLP中常用) &lt;/a&gt; 01/12/2021 &lt;a href=/ai/ai_algorithms/2021/12/01/gbdt.html title=AI&gt; 【AI】GBDT(Gradient Boosting Decision Tree)详解 &lt;/a&gt; 25/11/2021 &lt;a href=/ai/ai_algorithms/2021/11/25/ARIMA.html title=AI&gt; 【AI】Arima模型详解 &lt;/a&gt; 21/11/2021 &lt;a href=/ai/ai_basics/2021/11/21/term-vector.html title=AI&gt; 【AI】Term-Vector含义 &lt;/a&gt; 20/11/2021 &lt;a href=/ai/ai_basics/2021/11/20/softmax.html title=AI&gt; 【AI】Softmax函数 &lt;/a&gt; 19/11/2021 &lt;a href=/ai/ads/2021/11/19/Recommend.html title=AI&gt; 【Ad】Recommendation System Overview(推荐系统详解) &lt;/a&gt;]]></summary></entry><entry><title type="html">【AI】LLM Thoughts</title><link href="http://0.0.0.0:4000/ai/ai_basics/2023/05/08/llm-thought.html" rel="alternate" type="text/html" title="【AI】LLM Thoughts" /><published>2023-05-08T16:46:07+08:00</published><updated>2023-05-08T16:46:07+08:00</updated><id>http://0.0.0.0:4000/ai/ai_basics/2023/05/08/llm-thought</id><content type="html" xml:base="http://0.0.0.0:4000/ai/ai_basics/2023/05/08/llm-thought.html"><![CDATA[<h1 id="背景">背景</h1>

<p>可能是最近才开始关注Github上关于LLM和AIGC的更新，用日新月异来形容毫不过分。每天的Github trending都在变，而且绝大部分都是关于AI，非相关内容已经排不上trending了。</p>

<p>2016年AlphaGo出来下棋的时候，还没有体会到AI能够发展成为今天的样子，因为把棋下好这件事和日常生活提效，或者是人工智能取代人工这个构想之间的逻辑关系过于大，不光大头兵们，大佬们肯定也没想到，不然他们早就入局了，不至于OpenAI都做出来了才都纷纷带着自己的家底入场。</p>

<p>对于一个从事编程行业的人来说，哪些应该是我的关注点呢？</p>

<h1 id="breakdown">Breakdown</h1>

<p>首先，大模型大概率没法做。这个事情没有多少资本都没有入场券那种，不过我倒是很好奇，如果在现在的模型基础上做fine-tune，怎么做呢？不能有一个想法就去训练，成本太高了，肯定需要有一些快速试错和迭代的方法。这里我不太了解，后续关注一下准备再写一篇笔记。</p>

<p>其次，基于大模型能做什么？现在的垂直市场是绝对的蓝海，因为没有一个非常Dominant的公司在垂直领域做到了一个足够爆款的App，就算做出来了，这个门槛也比较低，举个最简单的例子，MidJourney现在多少人在用？Bing现在都集成了Dall-E了，每个人都能上去画图，谁还会用收费的MidJourney吗？我觉得时间长了肯定没人用了，除非能在速度和质量或者易用性上产生明显的差异。当然目前来看，Bing这里暂时还不太行。</p>

<p><img src="/assets/images/image-20230508204012732.png" alt="image-20230508204012732" /></p>

<p>那么关于垂类应用，最有前景的是什么？我大胆预测一下是数据预处理 + ChatGPT。为什么是数据预处理，因为OpenAI给了一个Token Limit，就是一次发送给OpenAI的字符数量是有限的，并且是收费的，发过去的内容越多，分析的成本就越高，比如大数据分析，文章摘要等。因此这里肯定是可以有护城河的东西，因为这里有了现实的限制，就连OpenAI自己也没办法发送无限多的Token给自己的大模型。这里很多技巧就可以用上了，展示三个应用，都是最近Github上非常火的开源项目。</p>

<ul>
  <li><a href="https://github.com/pashpashpash/vault-ai">vault-ai</a>： 通过上传自己的文档，建立自己的私人知识库。</li>
  <li><a href="https://github.com/bhaskatripathi/pdfGPT">pdfGPT</a>：上传PDF，然后可以对PDF进行提问。</li>
  <li>ChatGPT code interpreter插件：上传csv文件，告诉ChatGPT你想生成的结果，某种结论或者图片都可以，然后生成Python Code并运行，返回给用户处理的结果。（直接拿到Python Code自己运行也是可以的，参考这篇文章：<a href="https://mp.weixin.qq.com/s/4jNYSW-AVKcpXuhRlUAu1w">WeChat Article</a></li>
</ul>

<p>前两个的思路都是非常接近的，把输入的文档处理成embedding，然后把问题也encode成embedding，把问题的embedding和所有输入的embedding的进行相关性查找，找到最相关的前n个文本块，和问题一起封装成一个prompt发送给ChatGPT，从而能够回答你的问题。</p>

<p>PDFGPT的Flowchart</p>

<pre><code class="language-mermaid">flowchart TB
A[Input] --&gt; B[URL]
A -- Upload File manually --&gt; C[Parse PDF]
B --&gt; D[Parse PDF] -- Preprocess --&gt; E[Dynamic Text Chunks]
C -- Preprocess --&gt; E[Dynamic Text Chunks with citation history]
E --Fit--&gt;F[Generate text embedding with Deep Averaging Network Encoder on each chunk]
F -- Query --&gt; G[Get Top Results]
G -- K-Nearest Neighbour --&gt; K[Get Nearest Neighbour - matching citation references]
K -- Generate Prompt --&gt; H[Generate Answer]
H -- Output --&gt; I[Output]
</code></pre>

<p>最后一个是数据分析相关的，海量数据自然也是不能直接喂给ChatGPT的，因此OpenAI的思路就是读取一部分上传文件，csv读头部前几行即可，就能知道文件和什么相关，并且还有部分样例数据，随后根据用户的指令生成一些Python代码，执行代码并且把分析好的结果返回给用户。多了一些自动化，确实方便很多，这个能做比较小型的数据分析，大型的肯定无法通过上传文件做到。</p>

<p>我们可以看到，这两者的预处理都是有损的，并不是和大模型直接交互的。既然是有损的，那么谁的损失更少，谁的结果更精准，让用户花费时间更少，同时自己的成本更低，谁就更有机会胜出。并且大模型不好实时更新，recommender总可以吧，这都是多么成熟的领域了，集成个语言大模型到自己的工作流里面也不会有多困难。但本身这里还有个tradeoff，能给大模型更多的信息总是更好的，这也导致了这些方法都是Token hungry的。对于数据分析来说，生成的Python更准确，prompt的字数越少，越能胜出。</p>

<p>最后，知道这些了能做什么吗？我个人认为还是从实际需求入手，从自己的工作流入手，做一个特别垂直的事情。没有乔老爷那么敏锐的用户洞察力，自己工作流里遇到的问题，就是大家会遇到的问题。多了的地方只能通过聊天获取，但sizing有多大，这个就无法判断了。</p>

<h1 id="conclusion">Conclusion</h1>

<p>来自Bing的总结：</p>

<p>The text is about the recent developments in AI, especially large language models (LLM) and AI-generated content (AIGC). The author discusses the challenges and opportunities for programmers in this field. The author suggests that data preprocessing and chat-based applications are the most promising areas for innovation. The author also gives some examples of popular open-source projects that use LLM and AIGC, such as vault-ai, pdfGPT, and ChatGPT code interpreter. The author argues that both tools have some tradeoffs in terms of preprocessing, accuracy, cost and prompt length. The author suggests that the best way to use these tools is to find a specific and vertical problem from one’s own workflow.</p>

<blockquote>
  <p>这篇文章主要介绍了AI领域的最新进展，特别是大型语言模型(LLM)和AI生成内容(AIGC)。作者分析了这个领域中程序员面临的挑战和机遇。作者认为数据预处理和聊天式应用是最有创新潜力的领域。作者还举了一些使用LLM和AIGC的热门开源项目的例子，比如vaut-ai,pdfGPT,和ChatGPT code interpreter。作者指出这两种工具在预处理、准确性、成本和提示长度方面都有一些权衡。作者建议使用这些工具的最佳方法是找到一个自己工作流中的具体而垂直的问题。</p>
</blockquote>

<p><img src="/assets/images/image-20230508212354089.png" alt="image-20230508212354089" /></p>

<p><img src="/assets/images/image-20230508212219199.png" alt="image-20230508212219199" /></p>]]></content><author><name>Chengru Song</name></author><category term="[&quot;AI&quot;, &quot;AI_Basics&quot;]" /><category term="301-work-ai" /><summary type="html"><![CDATA[背景 可能是最近才开始关注Github上关于LLM和AIGC的更新，用日新月异来形容毫不过分。每天的Github trending都在变，而且绝大部分都是关于AI，非相关内容已经排不上trending了。 2016年AlphaGo出来下棋的时候，还没有体会到AI能够发展成为今天的样子，因为把棋下好这件事和日常生活提效，或者是人工智能取代人工这个构想之间的逻辑关系过于大，不光大头兵们，大佬们肯定也没想到，不然他们早就入局了，不至于OpenAI都做出来了才都纷纷带着自己的家底入场。 对于一个从事编程行业的人来说，哪些应该是我的关注点呢？ Breakdown 首先，大模型大概率没法做。这个事情没有多少资本都没有入场券那种，不过我倒是很好奇，如果在现在的模型基础上做fine-tune，怎么做呢？不能有一个想法就去训练，成本太高了，肯定需要有一些快速试错和迭代的方法。这里我不太了解，后续关注一下准备再写一篇笔记。 其次，基于大模型能做什么？现在的垂直市场是绝对的蓝海，因为没有一个非常Dominant的公司在垂直领域做到了一个足够爆款的App，就算做出来了，这个门槛也比较低，举个最简单的例子，MidJourney现在多少人在用？Bing现在都集成了Dall-E了，每个人都能上去画图，谁还会用收费的MidJourney吗？我觉得时间长了肯定没人用了，除非能在速度和质量或者易用性上产生明显的差异。当然目前来看，Bing这里暂时还不太行。 那么关于垂类应用，最有前景的是什么？我大胆预测一下是数据预处理 + ChatGPT。为什么是数据预处理，因为OpenAI给了一个Token Limit，就是一次发送给OpenAI的字符数量是有限的，并且是收费的，发过去的内容越多，分析的成本就越高，比如大数据分析，文章摘要等。因此这里肯定是可以有护城河的东西，因为这里有了现实的限制，就连OpenAI自己也没办法发送无限多的Token给自己的大模型。这里很多技巧就可以用上了，展示三个应用，都是最近Github上非常火的开源项目。 vault-ai： 通过上传自己的文档，建立自己的私人知识库。 pdfGPT：上传PDF，然后可以对PDF进行提问。 ChatGPT code interpreter插件：上传csv文件，告诉ChatGPT你想生成的结果，某种结论或者图片都可以，然后生成Python Code并运行，返回给用户处理的结果。（直接拿到Python Code自己运行也是可以的，参考这篇文章：WeChat Article 前两个的思路都是非常接近的，把输入的文档处理成embedding，然后把问题也encode成embedding，把问题的embedding和所有输入的embedding的进行相关性查找，找到最相关的前n个文本块，和问题一起封装成一个prompt发送给ChatGPT，从而能够回答你的问题。 PDFGPT的Flowchart flowchart TB A[Input] --&gt; B[URL] A -- Upload File manually --&gt; C[Parse PDF] B --&gt; D[Parse PDF] -- Preprocess --&gt; E[Dynamic Text Chunks] C -- Preprocess --&gt; E[Dynamic Text Chunks with citation history] E --Fit--&gt;F[Generate text embedding with Deep Averaging Network Encoder on each chunk] F -- Query --&gt; G[Get Top Results] G -- K-Nearest Neighbour --&gt; K[Get Nearest Neighbour - matching citation references] K -- Generate Prompt --&gt; H[Generate Answer] H -- Output --&gt; I[Output] 最后一个是数据分析相关的，海量数据自然也是不能直接喂给ChatGPT的，因此OpenAI的思路就是读取一部分上传文件，csv读头部前几行即可，就能知道文件和什么相关，并且还有部分样例数据，随后根据用户的指令生成一些Python代码，执行代码并且把分析好的结果返回给用户。多了一些自动化，确实方便很多，这个能做比较小型的数据分析，大型的肯定无法通过上传文件做到。 我们可以看到，这两者的预处理都是有损的，并不是和大模型直接交互的。既然是有损的，那么谁的损失更少，谁的结果更精准，让用户花费时间更少，同时自己的成本更低，谁就更有机会胜出。并且大模型不好实时更新，recommender总可以吧，这都是多么成熟的领域了，集成个语言大模型到自己的工作流里面也不会有多困难。但本身这里还有个tradeoff，能给大模型更多的信息总是更好的，这也导致了这些方法都是Token hungry的。对于数据分析来说，生成的Python更准确，prompt的字数越少，越能胜出。 最后，知道这些了能做什么吗？我个人认为还是从实际需求入手，从自己的工作流入手，做一个特别垂直的事情。没有乔老爷那么敏锐的用户洞察力，自己工作流里遇到的问题，就是大家会遇到的问题。多了的地方只能通过聊天获取，但sizing有多大，这个就无法判断了。 Conclusion 来自Bing的总结： The text is about the recent developments in AI, especially large language models (LLM) and AI-generated content (AIGC). The author discusses the challenges and opportunities for programmers in this field. The author suggests that data preprocessing and chat-based applications are the most promising areas for innovation. The author also gives some examples of popular open-source projects that use LLM and AIGC, such as vault-ai, pdfGPT, and ChatGPT code interpreter. The author argues that both tools have some tradeoffs in terms of preprocessing, accuracy, cost and prompt length. The author suggests that the best way to use these tools is to find a specific and vertical problem from one’s own workflow. 这篇文章主要介绍了AI领域的最新进展，特别是大型语言模型(LLM)和AI生成内容(AIGC)。作者分析了这个领域中程序员面临的挑战和机遇。作者认为数据预处理和聊天式应用是最有创新潜力的领域。作者还举了一些使用LLM和AIGC的热门开源项目的例子，比如vaut-ai,pdfGPT,和ChatGPT code interpreter。作者指出这两种工具在预处理、准确性、成本和提示长度方面都有一些权衡。作者建议使用这些工具的最佳方法是找到一个自己工作流中的具体而垂直的问题。]]></summary></entry><entry><title type="html">【Basics】Useful Prompts</title><link href="http://0.0.0.0:4000/work/basics/2023/04/18/useful-prompts.html" rel="alternate" type="text/html" title="【Basics】Useful Prompts" /><published>2023-04-18T14:44:07+08:00</published><updated>2023-04-18T14:44:07+08:00</updated><id>http://0.0.0.0:4000/work/basics/2023/04/18/useful-prompts</id><content type="html" xml:base="http://0.0.0.0:4000/work/basics/2023/04/18/useful-prompts.html"><![CDATA[<h1 id="prompt-techniques">Prompt Techniques</h1>

<h1 id="specific-prompts">Specific Prompts</h1>

<h2 id="scala-prompts">Scala Prompts</h2>

<ol>
  <li>Incremental Id assignment
    <ol>
      <li>suppose I have a rdd called edgeDF which has src_id, dst_id columns. Now I want to turn srd_id and dst_id into a single column called vertex_id. Each vertex_id is unique and assign a new column called id with incremental number. 
Finally, I want to assign the id column to src_id and dst_id in the original edgeDF. please write the scala program</li>
    </ol>
  </li>
</ol>]]></content><author><name>Chengru Song</name></author><category term="[&quot;work&quot;, &quot;basics&quot;]" /><category term="301-work-blog" /><category term="301-work-learning" /><summary type="html"><![CDATA[Prompt Techniques Specific Prompts Scala Prompts Incremental Id assignment suppose I have a rdd called edgeDF which has src_id, dst_id columns. Now I want to turn srd_id and dst_id into a single column called vertex_id. Each vertex_id is unique and assign a new column called id with incremental number. Finally, I want to assign the id column to src_id and dst_id in the original edgeDF. please write the scala program]]></summary></entry><entry><title type="html">【阅读】The Tyranny of Merit. 傲慢的精英</title><link href="http://0.0.0.0:4000/blog/read/2023/01/10/the-tyranny-of-merit.html" rel="alternate" type="text/html" title="【阅读】The Tyranny of Merit. 傲慢的精英" /><published>2023-01-10T13:41:58+08:00</published><updated>2023-01-10T13:41:58+08:00</updated><id>http://0.0.0.0:4000/blog/read/2023/01/10/the-tyranny-of-merit</id><content type="html" xml:base="http://0.0.0.0:4000/blog/read/2023/01/10/the-tyranny-of-merit.html"><![CDATA[<h1 id="综述">综述</h1>

<p>interesting，优绩至上理论竟然会带来很大的副作用，而且其中一个甚至和看起来毫不相干的信奉上帝有关。</p>

<p>信奉上帝相信我们做了善行，那么上帝会根据这个善行给予我们奖赏，反之会给我们惩罚。因此当一个人极度不幸的时候，很多人甚至怀疑这个人其实做过一些不为人知的恶行，这加深了对受难者的伤害。</p>

<!--more-->

<p>而且这种思想极度以人为中心，上帝大部分时间都在回应人类的行为。因此，即使是在上帝面前，我们也是在争取赢得自己的命运，这又是一层优绩主义带来的伤害。</p>

<p>这是美国人写的书，这本书分别讨论了在历史中优绩主义的提现和现代优绩主义的提现。在历史中，这种优绩主义在信奉善有善报恶有恶报的人中有更为明显的体现。现代略有不同，现代美国经常主张自己的“美国梦”，每个人都能通过努力实现阶级跃迁，文凭也逐渐成为了大家最后能接受的偏见，即拥有更好学历的人是通过自己的努力实现的跨越，因此他们可以接受自己成为社会的精英并享受这种文凭给自己带来的优待。虽然实际上当前已经有非常多的人在表达精英统治的不满。而很多人也表达了希望大学文凭成为一个分类机器的观点。</p>

<h1 id="分类机器">分类机器</h1>

<p>大学作为分类机器，赞同这个观点的人基本上都认同，不同大学教授的课程的质量本身并不具有在未来具有区分度，而进入这个大学本身，即获取大学的文凭，是具有区分度的。</p>

<p>当然，作者也是非常义正言辞的阐明了以此功用的大学文凭的不公平性。考虑到真实数字就是收入来自全美前1%的家庭占据了常青藤高校超过50%的名额，如果再去考虑这里面的深层逻辑，在美国有非常多的学生是通过捐款进入学校的，还有通过运动员身份进入学校的，这些运动往往都是高尔夫，赛艇，马术等运动，这些运动本身也是平常家庭很难负担的。同时看似平等的SAT，或者中国的高考，以一个分数作为分类标准的，同样非常具有不公平性。富人家庭的孩子能够在大学前教育花费超额的金钱进行培训，甚至是残疾人认证等，来获得不同程度的录取优惠。这同时也会加剧他们的傲慢—我是学习成绩好，智商高来到了这样的地方，这与我的家庭无关。但这同时也会让他们更加不快乐，因为相比于满足自己的求知欲而言，获得更高的分数是更重要的。</p>

<p>综合我看到的另一篇文章，一些具有相当高智商水平的人进入到一些无关紧要，与其本身具有的智商水平并不匹配的岗位时，我认为这有可能是有意而为之。愚民，让聪明人做无关紧要的事，让他们脱离社会，沉浸在自己的世界里，可能是统治者的手段之一。作者讨论的东西当然是很好的，公平是一个天然具有吸引力的词，它让人神往，因此公平意味着可以相信自己。但位置不同，往往看到的公平不同，一个奋斗一生并取得一定成就的人，往往不希望让自己的成就只在当代，他希望延续。这是一个根植于深处的想法，不这么想的人往往是异类。为自己的后台创造良好的生存环境，是一个我在自然界也可以看到的现象，我认为这是一种兽性，是一种原始的欲望，而克制原始的欲望是一件非常难的事情。当你有机会作为规则的制定者，但是你制定的规则，并不会为你带来丝毫的便利，我认为这并不现实。因此尽管我们知道问题的弊端在哪里，但每一个走上那个位置的人都没有做出那样的选择，即使做出了，也很难在延续下去。</p>

<p>那么在这种情况下，到底什么是好的，什么是能够让社会变好的呢？</p>

<h1 id="给工作以尊严">给工作以尊严</h1>

<p>对于不同的工作，我们很容易，甚至于是习惯于，给其分出三六九等。社会对于不同工作是有偏见的，例如水管工、农民工等，他们的工作是没有任何尊严的。有一个解决办法是不以优秀与否来赋予一份工作尊严，而是在道德标准上看这份工作对社会的贡献。例如清洁工，他们的存在让瘟疫的蔓延更加困难，因此当我们从道德角度来评判这一份工作，是应当对其付出更多认可与尊重的。</p>

<p>尽管我十分认同作者说的内容，但我想站位的不同带来了很多执行上阻力。对于能够制定政策的人来说，他们的目标真的是让社会变得更好，每个人都有过得体面的权利吗？虽然大家都声称自己执政的目标是这个，但社会资源的总量是有限的，普通人分到了更多，就意味着这些需要从富人嘴里吐出来。这一套价值观很难真的在当前社会运行，除非物质资源真的极大丰富。但这个条件也许是一个错误假设，因为总可以引导出一些稀缺而我们又看重的资源，存在于这个社会，这样我们就能付出自己非常多的东西来得到这个资源，而这个资源的稀缺性正是人为营造的，例如中国的房地产。没有了房地产可能变成了农产品，汽车，凡是可以带来垄断且必须的产业，都可以被人为弄成稀缺资源，并一茬一茬割韭菜，直到这个社会，人成了稀缺资源，再也没有为上层提供营养的新鲜血液，这时努力活下来的人们才有了资源重新分配机会。作者不喜欢说资源重新分配，因为每个人获得的不是资源，而是对社会的贡献和他人对这份贡献的认可。这种认可会更加合理的分配这些资源。</p>

<h1 id="写在最后">写在最后</h1>

<p>作者提出了一些很好的想法，但对于普通人而言，他们目前的最优策略仍然是想尽办法成为这个体系的优绩者，就像上一本书说的，如果你不能改变游戏规则，那最好加入一个每个人都倾向于在这个规则下正常竞争的游戏。</p>]]></content><author><name>Chengru Song</name></author><category term="[&quot;Blog&quot;, &quot;Read&quot;]" /><category term="301-life-blog" /><summary type="html"><![CDATA[综述 interesting，优绩至上理论竟然会带来很大的副作用，而且其中一个甚至和看起来毫不相干的信奉上帝有关。 信奉上帝相信我们做了善行，那么上帝会根据这个善行给予我们奖赏，反之会给我们惩罚。因此当一个人极度不幸的时候，很多人甚至怀疑这个人其实做过一些不为人知的恶行，这加深了对受难者的伤害。]]></summary></entry><entry><title type="html">【阅读】kknmd house</title><link href="http://0.0.0.0:4000/blog/read/2023/01/06/house.html" rel="alternate" type="text/html" title="【阅读】kknmd house" /><published>2023-01-06T13:41:58+08:00</published><updated>2023-01-06T13:41:58+08:00</updated><id>http://0.0.0.0:4000/blog/read/2023/01/06/house</id><content type="html" xml:base="http://0.0.0.0:4000/blog/read/2023/01/06/house.html"><![CDATA[<h1 id="kk关于房价的思考">KK关于房价的思考</h1>

<p>洞见事务背后的运行逻辑，是预测未来发展的重要因素。如果希望自己有一定的预测能力，需要对以下几点进行加强</p>

<ol>
  <li>掌握足够优质的信息来源；</li>
  <li>对经济运行有较为充足的知识储备；</li>
  <li>对历史的发展规律及其背后的原因有足够的了解；</li>
  <li>基于这些了解能够产出action items。</li>
</ol>

<h2 id="一些摘抄">一些摘抄</h2>

<p>关于房产投资的方向，也有几点心得：供大家参考：</p>

<p>一、坚决不能投资自己不熟悉的城市</p>

<p>二、坚决不投资中小城市，一般省会及计划单列以上城市问题都不大，但中小城市即使房价上涨也存在变现困难问题。</p>

<p>三、坚决不投资距离大城市较偏远的旅游城市，比如山东乳山之类的，几乎无法变现。</p>

<p>四、慎重投资大城市的郊区，除非价格绝对低。如果外来人口比较多，zf又有发展规划，且价格与城区相比有较大的价差，才可以考虑</p>]]></content><author><name>Chengru Song</name></author><category term="[&quot;Blog&quot;, &quot;Read&quot;]" /><category term="301-life-blog" /><summary type="html"><![CDATA[KK关于房价的思考 洞见事务背后的运行逻辑，是预测未来发展的重要因素。如果希望自己有一定的预测能力，需要对以下几点进行加强 掌握足够优质的信息来源； 对经济运行有较为充足的知识储备； 对历史的发展规律及其背后的原因有足够的了解； 基于这些了解能够产出action items。 一些摘抄 关于房产投资的方向，也有几点心得：供大家参考： 一、坚决不能投资自己不熟悉的城市 二、坚决不投资中小城市，一般省会及计划单列以上城市问题都不大，但中小城市即使房价上涨也存在变现困难问题。 三、坚决不投资距离大城市较偏远的旅游城市，比如山东乳山之类的，几乎无法变现。 四、慎重投资大城市的郊区，除非价格绝对低。如果外来人口比较多，zf又有发展规划，且价格与城区相比有较大的价差，才可以考虑]]></summary></entry><entry><title type="html">【Basics】Graphx</title><link href="http://0.0.0.0:4000/work/basics/2023/01/04/graphx.html" rel="alternate" type="text/html" title="【Basics】Graphx" /><published>2023-01-04T11:44:07+08:00</published><updated>2023-01-04T11:44:07+08:00</updated><id>http://0.0.0.0:4000/work/basics/2023/01/04/graphx</id><content type="html" xml:base="http://0.0.0.0:4000/work/basics/2023/01/04/graphx.html"><![CDATA[<h1 id="reference">Reference</h1>

<ol>
  <li>
    <table>
      <tbody>
        <tr>
          <td>Graphx pregel code example: [Processing Hierarchical Data using Spark Graphx Pregel API</td>
          <td>Qubole](https://www.qubole.com/blog/processing-hierarchical-data-using-spark-graphx-pregel-api)</td>
        </tr>
      </tbody>
    </table>
  </li>
  <li>graphx official pregel api: <a href="https://spark.apache.org/docs/latest/graphx-programming-guide.html#pregel-api">GraphX - Spark 3.3.1 Documentation (apache.org)</a></li>
  <li></li>
</ol>]]></content><author><name>Chengru Song</name></author><category term="[&quot;work&quot;, &quot;basics&quot;]" /><category term="301-work-blog" /><category term="301-work-learning" /><summary type="html"><![CDATA[Reference Graphx pregel code example: [Processing Hierarchical Data using Spark Graphx Pregel API Qubole](https://www.qubole.com/blog/processing-hierarchical-data-using-spark-graphx-pregel-api) graphx official pregel api: GraphX - Spark 3.3.1 Documentation (apache.org)]]></summary></entry><entry><title type="html">【阅读】Algorithms to Live By</title><link href="http://0.0.0.0:4000/blog/read/2022/12/31/algorithms-to-live-by.md.html" rel="alternate" type="text/html" title="【阅读】Algorithms to Live By" /><published>2022-12-31T13:41:58+08:00</published><updated>2022-12-31T13:41:58+08:00</updated><id>http://0.0.0.0:4000/blog/read/2022/12/31/algorithms-to-live-by.md</id><content type="html" xml:base="http://0.0.0.0:4000/blog/read/2022/12/31/algorithms-to-live-by.md.html"><![CDATA[<h1 id="algorithms-to-live-by">Algorithms to live by</h1>

<p>我觉得很多人应该更熟悉这本书的中文译名，算法之美。看到了微信读书上对于这个译名的评价：为了追求所谓的“信达雅”而没有正确翻译出题目想表达的主题，乍一看还以为是讲算法的，实际上是讲算法在日常生活中的应用的。并且后半部分的翻译好像是为了赶工，看上去并没有针对中国人的表达习惯进行优化，非常像是机翻的，这就导致了后面几章的内容读起来比较晦涩。因此为了避免这个情况，我直接选择读原版的英文版。这反而对于我比较友好，因为上学学算法的时候就是英文教材，很多argot是可以直接代入和理解的，比阅读中文版更能让我理解作者表达的原意。</p>

<p>Regardless of that，这本书讲的还是非常好的。通过算法引申到哪些现实问题其实是这个算法的真实映射，而如果算法本身能够提供解决问题的最优解，那么我们同样可以把这个策略用到我们的生活当中。</p>

<h2 id="abstract">Abstract</h2>

<p>This book is all about introducing how you can apply algorithms to your daily life decisions to make your life easier.</p>

<h2 id="optimal-stopping">Optimal stopping</h2>

<p>“the optimal solution takes the form of what we’ll call the <strong>Look-Then-Leap Rule:</strong> You set a predetermined amount of time for “looking”—that is, exploring your options, gathering data—in which you categorically don’t choose anyone, no matter how impressive. After that point, you enter the “leap” phase, prepared to instantly commit to anyone who outshines the best applicant you saw in the look phase.”</p>

<p><img src="/assets/images/image-20230107160653033.png" alt="image-20230107160653033" /></p>

<p><img src="https://s3-us-west-2.amazonaws.com/secure.notion-static.com/121ad852-5f33-4902-906d-8fd544e34380/8E109C18-FA7C-4FE3-A717-CCE2B943AEA8.png" alt="8E109C18-FA7C-4FE3-A717-CCE2B943AEA8.png" /></p>

<p>This principle applies to any situation where you get a series of offers and pay a cost to seek or wait for the next.</p>

<h2 id="exploreexploit">Explore/exploit</h2>

<p>A situation where you don’t know which restaurant to go to, the existing known good or undiscovered ones.</p>

<p>一个简单的策略</p>

<p><strong>Win-Stay, Lose-Shift algorithm</strong>: choose an arm at random, and keep pulling it as long as it keeps paying off. If the arm doesn’t pay off after a particular pull, then switch to the other one. Although this simple strategy is far from a complete solution, Robbins proved in 1952 that it performs reliably better than chance.</p>

<h2 id="the-gittins-index">The Gittins index</h2>

<p>假设每次探索成功后，得到结果满意程度是上次的90%。那么就会有下面这张表。</p>

<p><img src="/assets/images/image-20230107160711255.png" alt="image-20230107160711255" /></p>

<h2 id="cache">cache</h2>

<p>Cache在生活中会有一些可以优化工作流的场景，比如电脑桌面程序和网页，保留哪些，关掉哪些。哪些是可以保存在收藏夹的，哪些是可以不在收藏夹但是可以通过两个动作打开的。</p>

<p>这么看的话可以把收藏夹问题抽象成一个多级缓存问题。对于这个处理原则可以是</p>

<ol>
  <li>哪些是最经常打开的？</li>
  <li>哪些频率略低，可以花时间寻找的？</li>
  <li>哪些可以直接放到文档里面，用到了再去找文档的？</li>
</ol>

<p>First, when you are deciding what to keep and what to throw away, LRU is potentially a good principle to use</p>

<p>“Second, exploit geography. Make sure things are in whatever cache is closest to the place where they’re typically used.”</p>

<p>难以想象，其实社会对于事件的遗忘程度也是一个艾宾浩斯曲线。</p>

<h2 id="scheduling">Scheduling</h2>

<p><strong>基本概念</strong></p>

<ol>
  <li>DDL优先：当你的事情没有重要性排序的时候，你只需要将每一个事情按照到达次序处理，找到快要DDL的事情来做就行。</li>
  <li>Thrashing，当你的事情有优先级排序的时候，很多事情又同时在做，就会崩溃，你发现其实一直在做context swtich，最后其实什么也没做，在外界看来，这样甚至成了拖延症。</li>
  <li>最小时间分片。操作系统实际上有个最小分片，小于这个分片，系统除了不断做context swtich，什么计算任务也无法完成，所以这个最小时间分片是无法继续分割的，必须做完才能做另一个任务。</li>
</ol>

<p><strong>Takeaway</strong></p>

<ol>
  <li>确定自己的最小处理时间分片。在一个分片内，尽量只做一件事。“The moral is that you should try to stay on a single task as long as possible without decreasing your responsiveness below the minimum acceptable limit.”</li>
  <li>Interrupt coalesce. 把给你的中断尽可能合并，比如统一回复邮件。</li>
  <li>对于概念2，可以随机处理事情，而不需要一直卡着优先级排序处理，否则在计算优先级这里又需要花很多时间。</li>
</ol>

<h2 id="bayes-rule">Bayes Rule</h2>

<ol>
  <li>How to predict the probabilities if it happens only once?
    <ol>
      <li>“Count the number of times it has happened in the past plus one, then divide by the number of opportunities plus two. ” da</li>
    </ol>
  </li>
</ol>

<p>人们总是会对最近发生的事情记忆更深，因此，往往会给最近的事情加更多的权重，这会让本来有可能预测准的事情不准。因此要谨慎对待类似的事情。</p>

<h2 id="overfitting">Overfitting</h2>

<p>顾名思义，这章基本上就是在讲，如何能通过正则化，噪声等来避免过拟合带来的影响。</p>

<p>“how early to stop depends on the gap between what you can measure and what really matters.”</p>

<h2 id="relaxation">Relaxation</h2>

<p>这章有个很有意思的观点，relaxation在数学里面的典型代表就是Lagrangian方法，把所有的constraints都当成优化函数的一部分，成为一个新的函数。即利用限制条件来修正优化函数，通过限制条件的最终数值，求得参数值，就能得到原函数的最优解。</p>

<p>类似的事情就是在生活中，我们面对很多选择的时候会受到很多限制条件的束缚。例如必须要工资在多少以上的，工作的位置必须在什么地方，等等。但如果目标是最合适自己的工作，就可以问自己，如果所有工作的工资都一样，我会选择什么？</p>

<p>实际上，在很多时候一个合适的放缩能找到的答案就是最优解了。</p>

<h2 id="randomness">Randomness</h2>

<p>随机，有时候是解决untractable问题的良药。比方说公私钥机制里面的大整数分解问题，本身找到一个数字是哪两个质数的乘积是一个只能穷举的算法，但是如果你可以接受一些错误率，用一些随机的方法，可以在错误率很小的情况下进行验证。</p>

<h2 id="networking">Networking</h2>

<p>Networking里面的慢启动和拥塞避免算法竟然可以映射到职场里！</p>

<p><img src="/assets/images/image-20230107160731675.png" alt="image-20230107160731675" /></p>

<p>考虑这个拥塞避免算法的本质是什么：在不知道对面capacity的情况下，怎么能快速试出来？</p>

<p>在工作中，作为领导，不知道这个人上限是什么的情况下，怎么能快速试出来？</p>

<p>快速指数级升职，直到他无法做好目前的工作。然后降到当前职级的一半（或者工作负载的一半）然后再线性提升。</p>

<h2 id="game-theory">Game theory</h2>

<p>在博弈论里，有一些假设就是每个人都是理性人，大家会做出在当前对自己局面最好的选择。</p>

<p>尽管比较难以接受，即使我们现在全部都变成自动驾驶，有成熟的算法来帮我们规划，在最好的情况下，堵车只能比现在少$\frac{1}{3}$。</p>

<p>在一些不行的Game里面，无论一个人怎么努力，最好的结局是也只是到达一个这个游戏的均衡点。作者用了黑五商店开门举了例子，以往黑五商场开门时间都是早晨8点。但是突然商场A说自己改成凌晨0点开门，对于其他商场来说，必须也在0点开门，这样才能保证销量。因此所有的商场都在0点开门，而谁也没有多挣到钱。这就是一个bad Game。</p>

<p>因此给我们的启示是</p>

<p>“If changing strategies doesn’t help, you can try to change the game. And if that’s not possible, you can at least exercise some control about which games you choose to play. The road to hell is paved with intractable recursions, bad equilibria, and information cascades. Seek out games where honesty is the dominant strategy. Then just be yourself.”</p>]]></content><author><name>Chengru Song</name></author><category term="[&quot;Blog&quot;, &quot;Read&quot;]" /><category term="301-life-blog" /><summary type="html"><![CDATA[Algorithms to live by 我觉得很多人应该更熟悉这本书的中文译名，算法之美。看到了微信读书上对于这个译名的评价：为了追求所谓的“信达雅”而没有正确翻译出题目想表达的主题，乍一看还以为是讲算法的，实际上是讲算法在日常生活中的应用的。并且后半部分的翻译好像是为了赶工，看上去并没有针对中国人的表达习惯进行优化，非常像是机翻的，这就导致了后面几章的内容读起来比较晦涩。因此为了避免这个情况，我直接选择读原版的英文版。这反而对于我比较友好，因为上学学算法的时候就是英文教材，很多argot是可以直接代入和理解的，比阅读中文版更能让我理解作者表达的原意。 Regardless of that，这本书讲的还是非常好的。通过算法引申到哪些现实问题其实是这个算法的真实映射，而如果算法本身能够提供解决问题的最优解，那么我们同样可以把这个策略用到我们的生活当中。 Abstract This book is all about introducing how you can apply algorithms to your daily life decisions to make your life easier. Optimal stopping “the optimal solution takes the form of what we’ll call the Look-Then-Leap Rule: You set a predetermined amount of time for “looking”—that is, exploring your options, gathering data—in which you categorically don’t choose anyone, no matter how impressive. After that point, you enter the “leap” phase, prepared to instantly commit to anyone who outshines the best applicant you saw in the look phase.” This principle applies to any situation where you get a series of offers and pay a cost to seek or wait for the next. Explore/exploit A situation where you don’t know which restaurant to go to, the existing known good or undiscovered ones. 一个简单的策略 Win-Stay, Lose-Shift algorithm: choose an arm at random, and keep pulling it as long as it keeps paying off. If the arm doesn’t pay off after a particular pull, then switch to the other one. Although this simple strategy is far from a complete solution, Robbins proved in 1952 that it performs reliably better than chance. The Gittins index 假设每次探索成功后，得到结果满意程度是上次的90%。那么就会有下面这张表。 cache Cache在生活中会有一些可以优化工作流的场景，比如电脑桌面程序和网页，保留哪些，关掉哪些。哪些是可以保存在收藏夹的，哪些是可以不在收藏夹但是可以通过两个动作打开的。 这么看的话可以把收藏夹问题抽象成一个多级缓存问题。对于这个处理原则可以是 哪些是最经常打开的？ 哪些频率略低，可以花时间寻找的？ 哪些可以直接放到文档里面，用到了再去找文档的？ First, when you are deciding what to keep and what to throw away, LRU is potentially a good principle to use “Second, exploit geography. Make sure things are in whatever cache is closest to the place where they’re typically used.” 难以想象，其实社会对于事件的遗忘程度也是一个艾宾浩斯曲线。 Scheduling 基本概念 DDL优先：当你的事情没有重要性排序的时候，你只需要将每一个事情按照到达次序处理，找到快要DDL的事情来做就行。 Thrashing，当你的事情有优先级排序的时候，很多事情又同时在做，就会崩溃，你发现其实一直在做context swtich，最后其实什么也没做，在外界看来，这样甚至成了拖延症。 最小时间分片。操作系统实际上有个最小分片，小于这个分片，系统除了不断做context swtich，什么计算任务也无法完成，所以这个最小时间分片是无法继续分割的，必须做完才能做另一个任务。 Takeaway 确定自己的最小处理时间分片。在一个分片内，尽量只做一件事。“The moral is that you should try to stay on a single task as long as possible without decreasing your responsiveness below the minimum acceptable limit.” Interrupt coalesce. 把给你的中断尽可能合并，比如统一回复邮件。 对于概念2，可以随机处理事情，而不需要一直卡着优先级排序处理，否则在计算优先级这里又需要花很多时间。 Bayes Rule How to predict the probabilities if it happens only once? “Count the number of times it has happened in the past plus one, then divide by the number of opportunities plus two. ” da 人们总是会对最近发生的事情记忆更深，因此，往往会给最近的事情加更多的权重，这会让本来有可能预测准的事情不准。因此要谨慎对待类似的事情。 Overfitting 顾名思义，这章基本上就是在讲，如何能通过正则化，噪声等来避免过拟合带来的影响。 “how early to stop depends on the gap between what you can measure and what really matters.” Relaxation 这章有个很有意思的观点，relaxation在数学里面的典型代表就是Lagrangian方法，把所有的constraints都当成优化函数的一部分，成为一个新的函数。即利用限制条件来修正优化函数，通过限制条件的最终数值，求得参数值，就能得到原函数的最优解。 类似的事情就是在生活中，我们面对很多选择的时候会受到很多限制条件的束缚。例如必须要工资在多少以上的，工作的位置必须在什么地方，等等。但如果目标是最合适自己的工作，就可以问自己，如果所有工作的工资都一样，我会选择什么？ 实际上，在很多时候一个合适的放缩能找到的答案就是最优解了。 Randomness 随机，有时候是解决untractable问题的良药。比方说公私钥机制里面的大整数分解问题，本身找到一个数字是哪两个质数的乘积是一个只能穷举的算法，但是如果你可以接受一些错误率，用一些随机的方法，可以在错误率很小的情况下进行验证。 Networking Networking里面的慢启动和拥塞避免算法竟然可以映射到职场里！ 考虑这个拥塞避免算法的本质是什么：在不知道对面capacity的情况下，怎么能快速试出来？ 在工作中，作为领导，不知道这个人上限是什么的情况下，怎么能快速试出来？ 快速指数级升职，直到他无法做好目前的工作。然后降到当前职级的一半（或者工作负载的一半）然后再线性提升。 Game theory 在博弈论里，有一些假设就是每个人都是理性人，大家会做出在当前对自己局面最好的选择。 尽管比较难以接受，即使我们现在全部都变成自动驾驶，有成熟的算法来帮我们规划，在最好的情况下，堵车只能比现在少$\frac{1}{3}$。 在一些不行的Game里面，无论一个人怎么努力，最好的结局是也只是到达一个这个游戏的均衡点。作者用了黑五商店开门举了例子，以往黑五商场开门时间都是早晨8点。但是突然商场A说自己改成凌晨0点开门，对于其他商场来说，必须也在0点开门，这样才能保证销量。因此所有的商场都在0点开门，而谁也没有多挣到钱。这就是一个bad Game。 因此给我们的启示是 “If changing strategies doesn’t help, you can try to change the game. And if that’s not possible, you can at least exercise some control about which games you choose to play. The road to hell is paved with intractable recursions, bad equilibria, and information cascades. Seek out games where honesty is the dominant strategy. Then just be yourself.”]]></summary></entry><entry><title type="html">【Blog】年度回顾</title><link href="http://0.0.0.0:4000/life/blog/2022/12/31/past-year-reflection.html" rel="alternate" type="text/html" title="【Blog】年度回顾" /><published>2022-12-31T11:44:07+08:00</published><updated>2022-12-31T11:44:07+08:00</updated><id>http://0.0.0.0:4000/life/blog/2022/12/31/past-year-reflection</id><content type="html" xml:base="http://0.0.0.0:4000/life/blog/2022/12/31/past-year-reflection.html"><![CDATA[]]></content><author><name>Chengru Song</name></author><category term="[&quot;life&quot;, &quot;Blog&quot;]" /><category term="301-life-blog" /><summary type="html"><![CDATA[]]></summary></entry></feed>