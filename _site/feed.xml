<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="zh-CN"><generator uri="https://jekyllrb.com/" version="4.2.1">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" hreflang="zh-CN" /><updated>2024-06-01T14:12:54+08:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Chengru’s Blog</title><subtitle>A personal blog website for sharing of technology, reflection and branding. 
</subtitle><author><name>Chengru Song</name></author><entry><title type="html">【AI】The Platonic Representation Hypothesis</title><link href="http://localhost:4000/ai/ai_algorithms/vae/2024/05/24/platonic-representation.html" rel="alternate" type="text/html" title="【AI】The Platonic Representation Hypothesis" /><published>2024-05-24T14:34:07+08:00</published><updated>2024-05-24T14:34:07+08:00</updated><id>http://localhost:4000/ai/ai_algorithms/vae/2024/05/24/platonic-representation</id><content type="html" xml:base="http://localhost:4000/ai/ai_algorithms/vae/2024/05/24/platonic-representation.html"><![CDATA[<h1 id="好的模型都是类似的差的模型各有各的不幸">好的模型都是类似的，差的模型各有各的不幸</h1>

<blockquote>
  <ul>
    <li>时间：2024.05.13</li>
    <li>作者团队：MIT</li>
    <li>作者：Minyoung Huh,  Brian Cheung,  TongzhouWang, Phillip Isola</li>
    <li>有用指数：⭐️⭐️⭐️⭐️⭐️</li>
    <li>贡献程度：⭐️⭐️⭐️⭐️⭐️</li>
    <li>简单评价：用一个简洁的假设解释了很多问题，也预测了一些趋势，其中不乏较为详实的说明和严谨的推论。对于还在疑惑为什么大模型可以work的朋友，是一个很好的知识补充。</li>
  </ul>
</blockquote>

<h2 id="出发点">出发点</h2>

<p>科学的发展总是基于一些可证伪的假设的证明或推翻，例如经典力学解释不了的现象，在微观粒度会有量子力学的理论进行支撑。某些看似不可能改变的理论，也仅在某些时空维度下是成立的。这篇文章就提出了一个假设，即，<strong>好的大模型的表征是正在收敛的，包括不同模态的模型</strong>。</p>

<p>作者试图用以下流程证明这个假设</p>

<ol>
  <li>不同大模型表征底层数据的方式正在逐渐对齐；</li>
  <li>不同模态下，大模型衡量数据点距离的方法是非常相似的；</li>
  <li>这个收敛的方向，是向着一个通用的现实统计学模型。</li>
</ol>

<p>该假设的具象理解，假设现实是Z，那么X和Y两个模态的模型学习出来的embedding表征的内容是一致的。</p>

<p><img src="/assets/images/image-20240528093953188.png" alt="image-20240528093953188" /></p>

<h2 id="如何证明这个表征是收敛的">如何证明这个表征是收敛的</h2>

<p>作者只讨论可以产出embedding的模型。这里作者引入了一个<em>kernel</em>的概念。简单理解为模型区隔data points的本质特征，其实就是embedding。随后采用了<em>mutual nearest-neighbor metric</em>，用来衡量两个<em>kernel</em>之间的距离。作者试图证明两个东西</p>

<ol>
  <li>不同的神经网络正在收敛；</li>
  <li>收敛的这个现象在不同的模态下都是成立的。</li>
</ol>

<p><em>mutual nearest-neighbor metric(MNNM)</em>方法简单来说，就是把图文Pair$(x_i, y_i) \in \mathcal{X}$ 用不同模态的模型分别Encode成各自模态下的embedding，再把每一个mini-batch下的embedding进行聚类，看每个sample聚类出来的结果交集作为该Metrics衡量指标。例如，有图文对(1,a), (2, b), (3, c)，分别用LM处理123，Vision处理abc，处理完的embedding进行KNN聚类，1和2的距离和a与b的距离更接近，则表示两个kernel的距离更近。</p>

<p>他们介绍了一个工作，叫做model stitching（模型缝合）。给定两个训练好的模型$f$和$g$​​，假设他们都是由n层神经网络组成的，作者引入一个仿射层h，组成一个新的网络</p>

\[\begin{equation}
F = f_1 \circ \cdots \circ f_k \circ h \circ g_{k+1} \circ \cdots \circ g_m
\end{equation}\]

<p>如果这个新的网络$F$能有很好的表现，那么就证明$f$和$g$有能兼容的层。</p>

<p>他们的实验结果共有两个发现，</p>

<ol>
  <li>一个在ImageNet上训练的视觉网络能和一个在Places-365上训练的网络进行对齐并保持较好的表现；</li>
  <li>更靠前的层比靠后的层可交换性更强。</li>
</ol>

<p>作者找了一共78个Vision Model，在VTAB数据集上做了实验，得到了如下结果：表现优秀的模型聚类都在一起，表现差的模型都分布在不同的地方。</p>

<p><img src="/assets/images/image-20240529202644010.png" alt="image-20240529202644010" /></p>

<p>echo一下开头的结论：好的模型都是相似的，差的模型各有各的不幸。</p>

<h3 id="模型参数也是收敛的吗">模型参数也是收敛的吗？</h3>

<p>简单的答案是：能。作者没有展开讲，但有几个工作发现对于不同的参数空间，它们倾向于向某一个地方进行收敛。</p>

<h3 id="不同模态之间特征也是收敛的吗">不同模态之间特征也是收敛的吗？</h3>

<p>答案：是。有很多现象可以证明，例如LLaVA等多模态架构，仅用一个简单的Projector就能连接两个不同模态的大模型。更加能证明此处的一点是，参考BLIP-2的模型架构，它本质只做了prefix-tuning，没有更新LM的参数，就能够让LM识别图像特征。在语音领域同样有类似的现象。在下面的图里，图像或者语言模块的能力更强，两个模态能对齐的效果就越好，成本就越低。</p>

<p><img src="/assets/images/image-20240529210641490.png" alt="image-20240529210641490" /></p>

<h2 id="为什么表征在收敛">为什么表征在收敛</h2>

<p>我们从模型参数的本质入手，模型训练的本质是降低训练集的经验风险的参数集合+正则化。</p>

<p><img src="/assets/images/image-20240529211754625.png" alt="image-20240529211754625" /></p>

<p>因此，从以下几点进行切入</p>

<h3 id="从任务泛化性入手">从任务泛化性入手</h3>

<p>我们把解决任务的方法想象成一个集合，对于任务A来说，可能有N种方法解决问题，对任务B来说，有M种方法可以解决问题。所以对于一个可同时解决任务A和B的模型，这个方法是M和N的交集才能同时解决这两个问题。假设我们需要实现的任务越多，训练集越大，那么同时能解决这些问题的方法越少。</p>

<p><img src="/assets/images/image-20240529212623516.png" alt="image-20240529212623516" /></p>

<h3 id="从模型容量入手">从模型容量入手</h3>

<p>越大的模型参数空间越大，那么这个模型能够探索的空间也就越大，能力也就越强。</p>

<p><img src="/assets/images/image-20240529234243906.png" alt="image-20240529234243906" /></p>

<h3 id="从simplicity-bias思考">从Simplicity Bias思考</h3>

<p>模型在对数据集进行拟合的时候，是什么导致了让模型不至于学习到特别复杂的表征呢？答案很可能是Simplicity bias。简单来说，在学习过程中，模型会”偷懒“，故意选择最简单能完成任务的方式。所以，根据这个假设，如果一个大模型在不同的数据集上训练过，那么结果可能会向更小的解法空间收敛。（这同时也是奥卡姆剃刀原则的一个直接延伸版本。</p>

<p><img src="/assets/images/image-20240529235230631.png" alt="image-20240529235230631" /></p>

<h2 id="这个表征到底会收敛在何处">这个表征到底会收敛在何处？</h2>

<p>一个简单的结论是：表征会收敛于<strong>离散事件在真实世界的点互信息kernel</strong>。</p>

<p>打个比方，”猫“和”喵“作为两个数据点，经过一个好的大模型的kernel，得到的两个表征，他们的点互信息(Pointwise Mutual Information)应该是接近真实世界的这个数值。因为这是两个事件的点互信息，所以这个方程是可适用于所有模态，甚至是跨模态的。比如文本模态：”猫在叫“和一个视觉模态：”视频中猫在真实的叫“，这两个数据的点互信息应该是比较大的(和现实比较接近)。</p>

<p>两个事件的互信息概率分布定义</p>

\[\begin{equation}
P_{\text{coor}}(x_a, x_b) \propto \sum_{(t, t') : |t - t'| \leq T_{\text{window}}} \mathbb{P}(X_t = x_a, X_{t'} = x_b).
\end{equation}\]

<p>则两个表征之间的距离可以由以下公式计算</p>

\[\left\langle f_X(x_a), f_X(x_b) \right\rangle \approx \log \frac{\mathbb{P}(\text{pos} \mid x_a, x_b)}{\mathbb{P}(\text{neg} \mid x_a, x_b)} + \tilde{c}_X(x_a) \\
= \log \frac{P_{\text{coor}}(x_a \mid x_b)}{P_{\text{coor}}(x_a)} + c_X(x_a) \\
= K_{\text{PMI}}(x_a, x_b) + c_X(x_a)\]

<p>既然我们假设了一个好的大模型是真实世界的映射，因此实际上kernel的点互信息就是真实世界的点互信息。</p>

<h2 id="知道了表征收敛有什么基于此的推论">知道了表征收敛，有什么基于此的推论？</h2>

<ol>
  <li>Scaling是适用的，但不同方向的Scale有效率之分
    <ol>
      <li>Scaling law告诉我们，模型的表现随参数量，数据集大小和算力之间都有正相关，但是Scale的效率在不同阶段有不同表现。比如，数据集质量是明显短板时(例如不同模态之间的丰富程度不同)，Scale数据集质量要比Scale算力重要得多。</li>
    </ol>
  </li>
  <li>不同模态之间的数据集是可以共享的
    <ol>
      <li>如果你有N张图片和M条语料，又有假设是不同模态都可以反映现实，那么我们应该把图片数据和文本数据全部拿来训练。但他们之间可能有个换算关系，比如a个token代表b个pixels，少于这个关系可能无法作为可用训练的数据。</li>
    </ol>
  </li>
  <li>模态对齐在好的大模型之间应该是相当简单的函数
    <ol>
      <li>如果不同模态下，优秀的大模型有相似的底层表征，那么用尽可能简单的结构就可以把他们连接起来。</li>
    </ol>
  </li>
</ol>

<h2 id="这个假设的缺陷在哪里">这个假设的缺陷在哪里？</h2>

<ol>
  <li>有些东西只能用其固有模态表述。例如文本模态下根本不可能详细描述一张图片的所有细节，此时该假设为何成立？
    <ol>
      <li>作者argue的是，他们认同这个观点，但他们的假设比较严格建立在信息可严格双向映射情况，否则只能得到被其模态所能表达信息上限所限制的内容。</li>
    </ol>
  </li>
  <li>除了视觉和文本模态之外，很多模态并没有呈现出一种收敛的趋势
    <ol>
      <li>这里作者想了可能的原因，比如还没有探索如此深入等。</li>
    </ol>
  </li>
  <li>证明不同模型对齐能力的计算方法只能证明他们是相似的，但并不能证明是收敛的，更无法说明那个方向就是现实
    <ol>
      <li>这个是我提的，我自己的思考是，大语言模型用的很多数据集都是相同的，模型表现也是十分接近的，因此非常有可能只是模型相似，但并非都收敛到什么位置了。一切都是数据导致的。我对此的判断是（在物理学中很普遍的一个流程），我们可以先接受一个比较好的假设，这个假设用简洁的方程能解释很多问题，也能预测很多现象，在相当长的一段时间，如果该假设都能无法被证伪，那么我们可以先接受这个作为公理，直到有打破该公理的现象被观测到。</li>
    </ol>
  </li>
</ol>]]></content><author><name>Chengru Song</name></author><category term="[&quot;AI&quot;, &quot;AI_Algorithms&quot;, &quot;VAE&quot;]" /><category term="301-work-ai" /><summary type="html"><![CDATA[好的模型都是类似的，差的模型各有各的不幸 时间：2024.05.13 作者团队：MIT 作者：Minyoung Huh, Brian Cheung, TongzhouWang, Phillip Isola 有用指数：⭐️⭐️⭐️⭐️⭐️ 贡献程度：⭐️⭐️⭐️⭐️⭐️ 简单评价：用一个简洁的假设解释了很多问题，也预测了一些趋势，其中不乏较为详实的说明和严谨的推论。对于还在疑惑为什么大模型可以work的朋友，是一个很好的知识补充。 出发点 科学的发展总是基于一些可证伪的假设的证明或推翻，例如经典力学解释不了的现象，在微观粒度会有量子力学的理论进行支撑。某些看似不可能改变的理论，也仅在某些时空维度下是成立的。这篇文章就提出了一个假设，即，好的大模型的表征是正在收敛的，包括不同模态的模型。 作者试图用以下流程证明这个假设 不同大模型表征底层数据的方式正在逐渐对齐； 不同模态下，大模型衡量数据点距离的方法是非常相似的； 这个收敛的方向，是向着一个通用的现实统计学模型。 该假设的具象理解，假设现实是Z，那么X和Y两个模态的模型学习出来的embedding表征的内容是一致的。 如何证明这个表征是收敛的 作者只讨论可以产出embedding的模型。这里作者引入了一个kernel的概念。简单理解为模型区隔data points的本质特征，其实就是embedding。随后采用了mutual nearest-neighbor metric，用来衡量两个kernel之间的距离。作者试图证明两个东西 不同的神经网络正在收敛； 收敛的这个现象在不同的模态下都是成立的。 mutual nearest-neighbor metric(MNNM)方法简单来说，就是把图文Pair$(x_i, y_i) \in \mathcal{X}$ 用不同模态的模型分别Encode成各自模态下的embedding，再把每一个mini-batch下的embedding进行聚类，看每个sample聚类出来的结果交集作为该Metrics衡量指标。例如，有图文对(1,a), (2, b), (3, c)，分别用LM处理123，Vision处理abc，处理完的embedding进行KNN聚类，1和2的距离和a与b的距离更接近，则表示两个kernel的距离更近。 他们介绍了一个工作，叫做model stitching（模型缝合）。给定两个训练好的模型$f$和$g$​​，假设他们都是由n层神经网络组成的，作者引入一个仿射层h，组成一个新的网络 \[\begin{equation} F = f_1 \circ \cdots \circ f_k \circ h \circ g_{k+1} \circ \cdots \circ g_m \end{equation}\] 如果这个新的网络$F$能有很好的表现，那么就证明$f$和$g$有能兼容的层。 他们的实验结果共有两个发现， 一个在ImageNet上训练的视觉网络能和一个在Places-365上训练的网络进行对齐并保持较好的表现； 更靠前的层比靠后的层可交换性更强。 作者找了一共78个Vision Model，在VTAB数据集上做了实验，得到了如下结果：表现优秀的模型聚类都在一起，表现差的模型都分布在不同的地方。 echo一下开头的结论：好的模型都是相似的，差的模型各有各的不幸。 模型参数也是收敛的吗？ 简单的答案是：能。作者没有展开讲，但有几个工作发现对于不同的参数空间，它们倾向于向某一个地方进行收敛。 不同模态之间特征也是收敛的吗？ 答案：是。有很多现象可以证明，例如LLaVA等多模态架构，仅用一个简单的Projector就能连接两个不同模态的大模型。更加能证明此处的一点是，参考BLIP-2的模型架构，它本质只做了prefix-tuning，没有更新LM的参数，就能够让LM识别图像特征。在语音领域同样有类似的现象。在下面的图里，图像或者语言模块的能力更强，两个模态能对齐的效果就越好，成本就越低。 为什么表征在收敛 我们从模型参数的本质入手，模型训练的本质是降低训练集的经验风险的参数集合+正则化。 因此，从以下几点进行切入 从任务泛化性入手 我们把解决任务的方法想象成一个集合，对于任务A来说，可能有N种方法解决问题，对任务B来说，有M种方法可以解决问题。所以对于一个可同时解决任务A和B的模型，这个方法是M和N的交集才能同时解决这两个问题。假设我们需要实现的任务越多，训练集越大，那么同时能解决这些问题的方法越少。 从模型容量入手 越大的模型参数空间越大，那么这个模型能够探索的空间也就越大，能力也就越强。 从Simplicity Bias思考 模型在对数据集进行拟合的时候，是什么导致了让模型不至于学习到特别复杂的表征呢？答案很可能是Simplicity bias。简单来说，在学习过程中，模型会”偷懒“，故意选择最简单能完成任务的方式。所以，根据这个假设，如果一个大模型在不同的数据集上训练过，那么结果可能会向更小的解法空间收敛。（这同时也是奥卡姆剃刀原则的一个直接延伸版本。 这个表征到底会收敛在何处？ 一个简单的结论是：表征会收敛于离散事件在真实世界的点互信息kernel。 打个比方，”猫“和”喵“作为两个数据点，经过一个好的大模型的kernel，得到的两个表征，他们的点互信息(Pointwise Mutual Information)应该是接近真实世界的这个数值。因为这是两个事件的点互信息，所以这个方程是可适用于所有模态，甚至是跨模态的。比如文本模态：”猫在叫“和一个视觉模态：”视频中猫在真实的叫“，这两个数据的点互信息应该是比较大的(和现实比较接近)。 两个事件的互信息概率分布定义 \[\begin{equation} P_{\text{coor}}(x_a, x_b) \propto \sum_{(t, t') : |t - t'| \leq T_{\text{window}}} \mathbb{P}(X_t = x_a, X_{t'} = x_b). \end{equation}\] 则两个表征之间的距离可以由以下公式计算 \[\left\langle f_X(x_a), f_X(x_b) \right\rangle \approx \log \frac{\mathbb{P}(\text{pos} \mid x_a, x_b)}{\mathbb{P}(\text{neg} \mid x_a, x_b)} + \tilde{c}_X(x_a) \\ = \log \frac{P_{\text{coor}}(x_a \mid x_b)}{P_{\text{coor}}(x_a)} + c_X(x_a) \\ = K_{\text{PMI}}(x_a, x_b) + c_X(x_a)\] 既然我们假设了一个好的大模型是真实世界的映射，因此实际上kernel的点互信息就是真实世界的点互信息。 知道了表征收敛，有什么基于此的推论？ Scaling是适用的，但不同方向的Scale有效率之分 Scaling law告诉我们，模型的表现随参数量，数据集大小和算力之间都有正相关，但是Scale的效率在不同阶段有不同表现。比如，数据集质量是明显短板时(例如不同模态之间的丰富程度不同)，Scale数据集质量要比Scale算力重要得多。 不同模态之间的数据集是可以共享的 如果你有N张图片和M条语料，又有假设是不同模态都可以反映现实，那么我们应该把图片数据和文本数据全部拿来训练。但他们之间可能有个换算关系，比如a个token代表b个pixels，少于这个关系可能无法作为可用训练的数据。 模态对齐在好的大模型之间应该是相当简单的函数 如果不同模态下，优秀的大模型有相似的底层表征，那么用尽可能简单的结构就可以把他们连接起来。 这个假设的缺陷在哪里？ 有些东西只能用其固有模态表述。例如文本模态下根本不可能详细描述一张图片的所有细节，此时该假设为何成立？ 作者argue的是，他们认同这个观点，但他们的假设比较严格建立在信息可严格双向映射情况，否则只能得到被其模态所能表达信息上限所限制的内容。 除了视觉和文本模态之外，很多模态并没有呈现出一种收敛的趋势 这里作者想了可能的原因，比如还没有探索如此深入等。 证明不同模型对齐能力的计算方法只能证明他们是相似的，但并不能证明是收敛的，更无法说明那个方向就是现实 这个是我提的，我自己的思考是，大语言模型用的很多数据集都是相同的，模型表现也是十分接近的，因此非常有可能只是模型相似，但并非都收敛到什么位置了。一切都是数据导致的。我对此的判断是（在物理学中很普遍的一个流程），我们可以先接受一个比较好的假设，这个假设用简洁的方程能解释很多问题，也能预测很多现象，在相当长的一段时间，如果该假设都能无法被证伪，那么我们可以先接受这个作为公理，直到有打破该公理的现象被观测到。]]></summary></entry><entry><title type="html">【AI】VQ-VAE</title><link href="http://localhost:4000/ai/ai_algorithms/2024/05/19/vq-vae.html" rel="alternate" type="text/html" title="【AI】VQ-VAE" /><published>2024-05-19T16:37:07+08:00</published><updated>2024-05-19T16:37:07+08:00</updated><id>http://localhost:4000/ai/ai_algorithms/2024/05/19/vq-vae</id><content type="html" xml:base="http://localhost:4000/ai/ai_algorithms/2024/05/19/vq-vae.html"><![CDATA[<h1 id="vq-vae-详解">VQ-VAE 详解</h1>

<blockquote>
  <ul>
    <li>时间：2017.11.2</li>
    <li>作者团队：DeepMinds</li>
    <li>作者：Aaron van den Oord, Oriol Vinyals, Koray Kavukcuoglu</li>
    <li>有用指数：⭐️⭐️⭐️⭐️⭐️</li>
    <li>贡献程度：⭐️⭐️⭐️⭐️⭐️</li>
    <li>简单评价：开山鼻祖之作</li>
  </ul>
</blockquote>

<h2 id="背景知识">背景知识</h2>

<ol>
  <li>VAE详解：https://towardsdatascience.com/understanding-variational-autoencoders-vaes-f70510919f73</li>
  <li></li>
</ol>]]></content><author><name>Chengru Song</name></author><category term="[&quot;AI&quot;, &quot;AI_Algorithms&quot;]" /><category term="301-work-ai" /><summary type="html"><![CDATA[VQ-VAE 详解 时间：2017.11.2 作者团队：DeepMinds 作者：Aaron van den Oord, Oriol Vinyals, Koray Kavukcuoglu 有用指数：⭐️⭐️⭐️⭐️⭐️ 贡献程度：⭐️⭐️⭐️⭐️⭐️ 简单评价：开山鼻祖之作 背景知识 VAE详解：https://towardsdatascience.com/understanding-variational-autoencoders-vaes-f70510919f73]]></summary></entry><entry><title type="html">【AI】Make A Scene - Meta的T2I核心解析</title><link href="http://localhost:4000/ai/ai_algorithms/2024/05/19/make-a-scene.html" rel="alternate" type="text/html" title="【AI】Make A Scene - Meta的T2I核心解析" /><published>2024-05-19T16:37:07+08:00</published><updated>2024-05-19T16:37:07+08:00</updated><id>http://localhost:4000/ai/ai_algorithms/2024/05/19/make-a-scene</id><content type="html" xml:base="http://localhost:4000/ai/ai_algorithms/2024/05/19/make-a-scene.html"><![CDATA[<h1 id="meta-t2i方案解析">Meta T2I方案解析</h1>

<blockquote>
  <ul>
    <li>时间：2022.3.24</li>
    <li>作者团队：FAIR at Meta（Meta的AI研究团队）</li>
    <li>作者：Oran Gafni, Adam Polyak, Oron Ashual, Shelly Sheynin, Devi Parikh, Yaniv Taigman</li>
    <li>有用指数：⭐️</li>
    <li>贡献程度：⭐️</li>
    <li>简单评价：</li>
  </ul>
</blockquote>

<p>原文标题：<a href="https://arxiv.org/abs/2203.13131">Make-A-Scene: Scene-Based Text-to-Image Generation with Human Priors</a></p>

<h2 id="existing-gap">Existing Gap</h2>

<ol>
  <li>
    <p><strong>生成细节的可控性很低</strong>：有些控制能精准被语言描述，但有更多是很难被语言描述的，后者的控制非常难。</p>
  </li>
  <li><strong>感知差异</strong>：生成人脸时候优化的目标和结果之间有Gap，导致生成目标和实际结果差异很大</li>
  <li><strong>图片分辨率太小</strong>：当前SoTA还是256x256分辨率大小的。他们想把这个Size增大到512x512。</li>
</ol>]]></content><author><name>Chengru Song</name></author><category term="[&quot;AI&quot;, &quot;AI_Algorithms&quot;]" /><category term="301-work-ai" /><summary type="html"><![CDATA[Meta T2I方案解析 时间：2022.3.24 作者团队：FAIR at Meta（Meta的AI研究团队） 作者：Oran Gafni, Adam Polyak, Oron Ashual, Shelly Sheynin, Devi Parikh, Yaniv Taigman 有用指数：⭐️ 贡献程度：⭐️ 简单评价： 原文标题：Make-A-Scene: Scene-Based Text-to-Image Generation with Human Priors Existing Gap 生成细节的可控性很低：有些控制能精准被语言描述，但有更多是很难被语言描述的，后者的控制非常难。 感知差异：生成人脸时候优化的目标和结果之间有Gap，导致生成目标和实际结果差异很大 图片分辨率太小：当前SoTA还是256x256分辨率大小的。他们想把这个Size增大到512x512。]]></summary></entry><entry><title type="html">【AI】Variational Auto Encoder</title><link href="http://localhost:4000/ai/ai_algorithms/vae/2024/05/19/vae.html" rel="alternate" type="text/html" title="【AI】Variational Auto Encoder" /><published>2024-05-19T16:37:07+08:00</published><updated>2024-05-19T16:37:07+08:00</updated><id>http://localhost:4000/ai/ai_algorithms/vae/2024/05/19/vae</id><content type="html" xml:base="http://localhost:4000/ai/ai_algorithms/vae/2024/05/19/vae.html"><![CDATA[<h1 id="vae---图像生成的transformer">VAE - 图像生成的Transformer</h1>

<blockquote>
  <ul>
    <li>时间：</li>
    <li>作者团队：</li>
    <li>作者：</li>
    <li>有用指数：⭐️⭐️⭐️⭐️⭐️</li>
    <li>贡献程度：⭐️⭐️⭐️⭐️⭐️</li>
    <li>简单评价：开山鼻祖之作</li>
  </ul>
</blockquote>]]></content><author><name>Chengru Song</name></author><category term="[&quot;AI&quot;, &quot;AI_Algorithms&quot;, &quot;VAE&quot;]" /><category term="301-work-ai" /><summary type="html"><![CDATA[VAE - 图像生成的Transformer 时间： 作者团队： 作者： 有用指数：⭐️⭐️⭐️⭐️⭐️ 贡献程度：⭐️⭐️⭐️⭐️⭐️ 简单评价：开山鼻祖之作]]></summary></entry><entry><title type="html">【AI】Chameleon - Meta全模态大模型</title><link href="http://localhost:4000/ai/ai_algorithms/2024/05/18/chamelon.html" rel="alternate" type="text/html" title="【AI】Chameleon - Meta全模态大模型" /><published>2024-05-18T11:21:07+08:00</published><updated>2024-05-18T11:21:07+08:00</updated><id>http://localhost:4000/ai/ai_algorithms/2024/05/18/chamelon</id><content type="html" xml:base="http://localhost:4000/ai/ai_algorithms/2024/05/18/chamelon.html"><![CDATA[<h1 id="chameleon-meta全模态大模型">Chameleon Meta全模态大模型</h1>

<blockquote>
  <ul>
    <li>时间：2024.5.17</li>
    <li>作者团队：FAIR at Meta（Meta的AI研究团队）</li>
    <li>作者：</li>
    <li>有用指数：⭐️⭐️⭐️⭐️⭐️</li>
    <li>贡献程度：⭐️⭐️⭐️</li>
    <li>简单评价：</li>
  </ul>
</blockquote>

<p>多模态大模型，常规来看有两个思路。第一是之前流行的，把==视觉模块对齐到语言模块==，一般会用到一个预训练的视觉特征提取模型和一个语言模型，中间用一些全连接或者Q-former结果连接，使用一些图像和文本对数据集进行对齐；第二种是全模态统一大模型，神经网络的输入和输出都使用相同的tokenizer，用一个Transformer的结构来处理这些全模态token，并输出多模态token来解码。</p>

<p>第二种显然是大家正在发力的方向。例如今天的这篇文章，Chameleon。GPT-4o也是类似的架构，只是GPT-4o的模态会更全一些，包含TTS的token。</p>

<h2 id="existing-gap">Existing Gap</h2>

<p>分开对待语言和视觉模块是严重限制了模型输出多模态的结果。</p>

<h2 id="contribution">Contribution</h2>

<ol>
  <li>把文本和图像的tokenizer进行了统一，方法是把图片量化成离散的token。</li>
</ol>

<p>这里我有一个疑问</p>

<p><em>文本的tokenizatioin比较好理解，因为语料库的大小是有限的，所有单词或者token的数量也就几万，组成复杂文本的就只有这几万个基本元素。但组成图片的基本元素有哪些？图像的tokenization该怎么做？</em></p>

<h2 id="method">Method</h2>

<ol>
  <li>场景评估：作者想focus在三个场景，纯文本，单图文pair和图文interleaved形式</li>
  <li>图像Tokenizer选取：</li>
</ol>

<p>图像Tokenizer的选择需要了解之前Meta的一个工作：<a href="https://arxiv.org/pdf/2203.13131">Make-A-Scene: Scene-Based Text-to-Image Generation with Human Priors</a>，简单介绍下。</p>]]></content><author><name>Chengru Song</name></author><category term="[&quot;AI&quot;, &quot;AI_Algorithms&quot;]" /><category term="301-work-ai" /><summary type="html"><![CDATA[Chameleon Meta全模态大模型 时间：2024.5.17 作者团队：FAIR at Meta（Meta的AI研究团队） 作者： 有用指数：⭐️⭐️⭐️⭐️⭐️ 贡献程度：⭐️⭐️⭐️ 简单评价： 多模态大模型，常规来看有两个思路。第一是之前流行的，把==视觉模块对齐到语言模块==，一般会用到一个预训练的视觉特征提取模型和一个语言模型，中间用一些全连接或者Q-former结果连接，使用一些图像和文本对数据集进行对齐；第二种是全模态统一大模型，神经网络的输入和输出都使用相同的tokenizer，用一个Transformer的结构来处理这些全模态token，并输出多模态token来解码。 第二种显然是大家正在发力的方向。例如今天的这篇文章，Chameleon。GPT-4o也是类似的架构，只是GPT-4o的模态会更全一些，包含TTS的token。 Existing Gap 分开对待语言和视觉模块是严重限制了模型输出多模态的结果。 Contribution 把文本和图像的tokenizer进行了统一，方法是把图片量化成离散的token。 这里我有一个疑问 文本的tokenizatioin比较好理解，因为语料库的大小是有限的，所有单词或者token的数量也就几万，组成复杂文本的就只有这几万个基本元素。但组成图片的基本元素有哪些？图像的tokenization该怎么做？ Method 场景评估：作者想focus在三个场景，纯文本，单图文pair和图文interleaved形式 图像Tokenizer选取： 图像Tokenizer的选择需要了解之前Meta的一个工作：Make-A-Scene: Scene-Based Text-to-Image Generation with Human Priors，简单介绍下。]]></summary></entry><entry><title type="html">【AI】GPT-4o详解</title><link href="http://localhost:4000/ai/ai_algorithms/2024/05/16/gpt4o.html" rel="alternate" type="text/html" title="【AI】GPT-4o详解" /><published>2024-05-16T14:51:07+08:00</published><updated>2024-05-16T14:51:07+08:00</updated><id>http://localhost:4000/ai/ai_algorithms/2024/05/16/gpt4o</id><content type="html" xml:base="http://localhost:4000/ai/ai_algorithms/2024/05/16/gpt4o.html"><![CDATA[<h1 id="gpt-4o-分析">GPT-4o 分析</h1>]]></content><author><name>Chengru Song</name></author><category term="[&quot;AI&quot;, &quot;AI_Algorithms&quot;]" /><category term="301-work-ai" /><summary type="html"><![CDATA[GPT-4o 分析]]></summary></entry><entry><title type="html">【AI】MiniGPT-4详解</title><link href="http://localhost:4000/ai/ai_algorithms/2024/05/16/minigpt-4.html" rel="alternate" type="text/html" title="【AI】MiniGPT-4详解" /><published>2024-05-16T00:42:07+08:00</published><updated>2024-05-16T00:42:07+08:00</updated><id>http://localhost:4000/ai/ai_algorithms/2024/05/16/minigpt-4</id><content type="html" xml:base="http://localhost:4000/ai/ai_algorithms/2024/05/16/minigpt-4.html"><![CDATA[<h1 id="minigpt-4-详解">MiniGPT-4 详解</h1>

<blockquote>
  <ul>
    <li>时间：2023.4.20</li>
    <li>作者团队：King Abdullah University of Science and Technology</li>
    <li>作者：一作朱德尧，现在字节，一直做的都是视频/图像内容理解相关工作。</li>
    <li>有用指数：⭐️⭐️⭐️⭐️⭐️</li>
    <li>贡献程度：⭐️⭐️⭐️</li>
    <li>简单评价：这篇文章的方法没什么特别大创新，但是做了非常多实验，总结了非常多的经验，虽然有选取对自己有利的评估结果的嫌疑，但瑕不掩瑜，Ablation做的还是非常好的，对未来发展方向有两个特别大的Learning
      <ol>
        <li>==高质量数据非常关键，质量 » 数量==；</li>
        <li>==模态对齐模块不一定要复杂，数据才是关键==。</li>
      </ol>
    </li>
  </ul>
</blockquote>

<p>原文地址：https://arxiv.org/abs/2304.10592</p>

<h2 id="existing-gap">Existing Gap</h2>

<p>23年早期的文章，还在探索GPT-4v的图像语言能力的来源，他们的目标就是尽可能复现GPT-4V的图片理解语言能力。</p>

<h2 id="contribution">Contribution</h2>

<ol>
  <li>用Vicuna这些有强大语言能力的大模型来对齐视觉特征，能够有效激发Image understanding的能力；</li>
  <li>一个projection layer就能实现很强大的功能；</li>
  <li>==使用短的Image caption数据来对齐视觉特征是不够的，必须使用更多小的但是细节更丰富的description才能远远提升其能力。==</li>
</ol>

<h2 id="method">Method</h2>

<p>作者提出了一个非常简单但有效的结构：直接在BLIP-2的Q-former后面接一个projection layer，Training的时候只更新这个小projection layer的参数，其他视觉模块和语言模块都是完全不更新的。</p>

<p><img src="/assets/images/image-20240516144408009.png" alt="image-20240516144408009" /></p>

<blockquote>
  <p>[!NOTE]</p>

  <h4 id="几个疑问">几个疑问</h4>

  <table>
    <thead>
      <tr>
        <th>疑问</th>
        <th>回答</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td>只用projection layer方法的多模态在LLaVA中试验过了，是比较有用的，但LLaVA还更新了LM的参数，这个没有，性能上会有差距吗？</td>
        <td>个人认为在图像多轮对话上的能力肯定是逊色于LLaVA的，因为LLaVA用图片多轮对话训练过LM，肯定是有性能提升的。但是图像理解能力可能差距不大，或者MiniGPT-4还更好点。</td>
      </tr>
    </tbody>
  </table>

</blockquote>

<h3 id="训练过程">训练过程</h3>

<ol>
  <li>第一阶段
    <ol>
      <li>用Image&lt;&gt;caption pairs进行训练，==只更新Linear projector的参数==。这步训练完发现模型能够给出一些合理的返回，但是会有<strong>重复说话，句子碎片化，输出不相干</strong>等问题(这也是多模态大模型的常见问题了)。</li>
      <li>为了解决这个问题，作者首先进行了==更多的数据构造==。构造主要是产出对==图片更详细的描述==，而不是简单的caption，可以理解为，作者在构造image&lt;&gt;description pairs。构造的方法有两种
        <ol>
          <li>基于之前训练好的，能力不太行的模型版本，用==多轮对话的方法==让模型继续生成结果，直至完整。多轮对话就是在模型第一轮回复以后，在第二轮加上continue这个prompt，就像这样：<code class="language-plaintext highlighter-rouge">###Human: Continue ###Assistant:</code></li>
          <li>基于生成更详细描述，再对这部分数据用GPT-3.5后处理，主要是为了修复里面可能产生的语法错误等。prompt：<code class="language-plaintext highlighter-rouge">Fix the error in the given paragraph. Remove any repeating sentences, meaningless characters, not English sentences, and so on. Remove unnecessary repetition. Rewrite any incomplete sentences. Return directly the results without explanation. Return directly the input paragraph if it is already correct without explanation.</code></li>
        </ol>
      </li>
    </ol>
  </li>
  <li>第二阶段：
    <ol>
      <li>做完以上两步后，作者进行了第二阶段的fine-tuning，这里也是优化Generation Loss，用<code class="language-plaintext highlighter-rouge">describe this image to me</code>作为prompt进行调整，依然只训练Linear Projector。做完之后效果有显著提升。</li>
    </ol>
  </li>
</ol>

<h2 id="实验结果">实验结果</h2>

<ol>
  <li>
    <p>stage-2之后的效果：<img src="/assets/images/image-20240516214315387.png" alt="image-20240516214315387" /></p>
  </li>
  <li>那么这个stage-2的效果提升，对其他模型是否有效呢？由于作者是基于BLIP-2的架构进行的，那BLIP-2是否有类似提升呢？作者表示这里有提升，但是benefit没有MiniGPT-4大，这里非常有意思，==作者没有进行Quantitative的分析，只进行了qualitive的分析==。可能是cherry-pick了一些对自己有利的结果展示，BLIP-2也有很大的提升。</li>
  <li>那么除了fine-tune新加的Linear layer，fine-tune其他的模块有用吗？作者设计了三个实验
    <ol>
      <li>移除Q-former，直接用Projector map ViT的特征到Vicuna；</li>
      <li>用三层MLP，而不是一层；</li>
      <li>Fine-tune Projector的同时也fine-tune Q-former</li>
      <li>结果如下：<img src="/assets/images/image-20240516215327438.png" alt="image-20240516215327438" /></li>
    </ol>
  </li>
</ol>

<p>这个结果非常有意思，说明</p>

<ol>
  <li>复杂的Q-former结构对alignment帮助不一定大，一层MLP Projector足矣；</li>
  <li>深层MLP映射不一定有用；</li>
  <li>原本Align过的Q-former如果在小的数据集上fine-tune，效果甚至可能适得其反。</li>
</ol>

<h2 id="总结">总结</h2>

<p>总的来说，这篇文章的方法没什么特别大创新，但是做了非常多实验，总结了非常多的经验，虽然有选取对自己有利的评估结果的嫌疑，但瑕不掩瑜，Ablation做的还是非常好的，对未来发展方向有两个特别大的Learning</p>

<ol>
  <li>==高质量数据非常关键，质量 » 数量==；</li>
  <li>==模态对齐模块不一定要复杂，数据才是关键==。</li>
</ol>]]></content><author><name>Chengru Song</name></author><category term="[&quot;AI&quot;, &quot;AI_Algorithms&quot;]" /><category term="301-work-ai" /><summary type="html"><![CDATA[MiniGPT-4 详解 时间：2023.4.20 作者团队：King Abdullah University of Science and Technology 作者：一作朱德尧，现在字节，一直做的都是视频/图像内容理解相关工作。 有用指数：⭐️⭐️⭐️⭐️⭐️ 贡献程度：⭐️⭐️⭐️ 简单评价：这篇文章的方法没什么特别大创新，但是做了非常多实验，总结了非常多的经验，虽然有选取对自己有利的评估结果的嫌疑，但瑕不掩瑜，Ablation做的还是非常好的，对未来发展方向有两个特别大的Learning ==高质量数据非常关键，质量 » 数量==； ==模态对齐模块不一定要复杂，数据才是关键==。 原文地址：https://arxiv.org/abs/2304.10592 Existing Gap 23年早期的文章，还在探索GPT-4v的图像语言能力的来源，他们的目标就是尽可能复现GPT-4V的图片理解语言能力。 Contribution 用Vicuna这些有强大语言能力的大模型来对齐视觉特征，能够有效激发Image understanding的能力； 一个projection layer就能实现很强大的功能； ==使用短的Image caption数据来对齐视觉特征是不够的，必须使用更多小的但是细节更丰富的description才能远远提升其能力。== Method 作者提出了一个非常简单但有效的结构：直接在BLIP-2的Q-former后面接一个projection layer，Training的时候只更新这个小projection layer的参数，其他视觉模块和语言模块都是完全不更新的。 [!NOTE] 几个疑问 疑问 回答 只用projection layer方法的多模态在LLaVA中试验过了，是比较有用的，但LLaVA还更新了LM的参数，这个没有，性能上会有差距吗？ 个人认为在图像多轮对话上的能力肯定是逊色于LLaVA的，因为LLaVA用图片多轮对话训练过LM，肯定是有性能提升的。但是图像理解能力可能差距不大，或者MiniGPT-4还更好点。 训练过程 第一阶段 用Image&lt;&gt;caption pairs进行训练，==只更新Linear projector的参数==。这步训练完发现模型能够给出一些合理的返回，但是会有重复说话，句子碎片化，输出不相干等问题(这也是多模态大模型的常见问题了)。 为了解决这个问题，作者首先进行了==更多的数据构造==。构造主要是产出对==图片更详细的描述==，而不是简单的caption，可以理解为，作者在构造image&lt;&gt;description pairs。构造的方法有两种 基于之前训练好的，能力不太行的模型版本，用==多轮对话的方法==让模型继续生成结果，直至完整。多轮对话就是在模型第一轮回复以后，在第二轮加上continue这个prompt，就像这样：###Human: Continue ###Assistant: 基于生成更详细描述，再对这部分数据用GPT-3.5后处理，主要是为了修复里面可能产生的语法错误等。prompt：Fix the error in the given paragraph. Remove any repeating sentences, meaningless characters, not English sentences, and so on. Remove unnecessary repetition. Rewrite any incomplete sentences. Return directly the results without explanation. Return directly the input paragraph if it is already correct without explanation. 第二阶段： 做完以上两步后，作者进行了第二阶段的fine-tuning，这里也是优化Generation Loss，用describe this image to me作为prompt进行调整，依然只训练Linear Projector。做完之后效果有显著提升。 实验结果 stage-2之后的效果： 那么这个stage-2的效果提升，对其他模型是否有效呢？由于作者是基于BLIP-2的架构进行的，那BLIP-2是否有类似提升呢？作者表示这里有提升，但是benefit没有MiniGPT-4大，这里非常有意思，==作者没有进行Quantitative的分析，只进行了qualitive的分析==。可能是cherry-pick了一些对自己有利的结果展示，BLIP-2也有很大的提升。 那么除了fine-tune新加的Linear layer，fine-tune其他的模块有用吗？作者设计了三个实验 移除Q-former，直接用Projector map ViT的特征到Vicuna； 用三层MLP，而不是一层； Fine-tune Projector的同时也fine-tune Q-former 结果如下： 这个结果非常有意思，说明 复杂的Q-former结构对alignment帮助不一定大，一层MLP Projector足矣； 深层MLP映射不一定有用； 原本Align过的Q-former如果在小的数据集上fine-tune，效果甚至可能适得其反。 总结 总的来说，这篇文章的方法没什么特别大创新，但是做了非常多实验，总结了非常多的经验，虽然有选取对自己有利的评估结果的嫌疑，但瑕不掩瑜，Ablation做的还是非常好的，对未来发展方向有两个特别大的Learning ==高质量数据非常关键，质量 » 数量==； ==模态对齐模块不一定要复杂，数据才是关键==。]]></summary></entry><entry><title type="html">【AI】Internlm-xcomposer 2</title><link href="http://localhost:4000/ai/ai_algorithms/2024/05/11/internlmxc2.html" rel="alternate" type="text/html" title="【AI】Internlm-xcomposer 2" /><published>2024-05-11T15:35:07+08:00</published><updated>2024-05-11T15:35:07+08:00</updated><id>http://localhost:4000/ai/ai_algorithms/2024/05/11/internlmxc2</id><content type="html" xml:base="http://localhost:4000/ai/ai_algorithms/2024/05/11/internlmxc2.html"><![CDATA[<h1 id="internlm-xcomposer-2结构详解">Internlm-xcomposer 2结构详解</h1>

<hr />

<blockquote>
  <ul>
    <li>有用指数：⭐️⭐️</li>
    <li>贡献程度：⭐️</li>
    <li>简单评价：引入了新的架构进行模态对齐，该模块的intuitive没讲清楚，没有详实的ablation分析，现有问题分析不够明确，改进不够有针对性，看上去是没想清楚的暴力尝试，比如提升分辨率可能有用？改对齐模块可能有用？最终可能是都没用，就是暴力加数据最有用。</li>
  </ul>
</blockquote>

<hr />

<p>原文地址：https://arxiv.org/pdf/2401.16420</p>

<h2 id="existing-gap">Existing Gap</h2>

<p>作者认为，现在的VLLM任务还有一个没怎么探索明白的方向，就是怎么做模态对齐。目前业界对齐模态时对待Visual Tokens通常有两种方法，作者认为他们各自都有弊端。</p>

<ol>
  <li>平等对待Visual Token和Language Token，这样做忽略了两个模态之间本身的巨大差异；</li>
  <li>认为不同模态是不同的实体，这样做会增加对齐成本。</li>
</ol>

<h2 id="contribution">Contribution</h2>

<p>基于以上的Gap，作者认为自己做了两点贡献。</p>

<ol>
  <li>提出了一个PLoRA的结构，把image Token经过LoRA Module转化后再和Language Token结合在一起，这样就没有简单的把Image Token和Language Token进行混合；</li>
  <li>组织了高质量，丰富的训练数据集。</li>
</ol>

<p>看到这里个人有两个疑问</p>

<ol>
  <li>大部分工作（LLaVA除外）都没有把Image Token和Language Token等价，都还是使用了Q-former等结构对齐过的，所以文中提到的<strong>忽略模态差异，到底是什么差异？谁在忽略差异</strong>？</li>
  <li>第二，即使这个差异被忽略了，==有业界共识或者实验表明，这个差异是不可忽略的吗==？LLaVA这个结构真的不好吗？如果不好为什么Benchmark呈现的结果还行？</li>
</ol>

<h2 id="method">Method</h2>

<h4 id="作者提出的plora结构">作者提出的PLoRA结构</h4>

<p>这个PLoRA结构，把原来LM的每一层预训练的权重换成了Low Rank  Adaptor
\(\begin{align*}
\hat{x}_t &amp;= W_0x_t + B_0 \\
\hat{x}_v &amp;= W_0x_v + W_BW_Ax_v + B_0 \\
\hat{x} &amp;= [\hat{x}_v, \hat{x}_t]
\end{align*}\)</p>

<blockquote>
  <p>[!TIP]</p>

  <h4 id="拓展知识">拓展知识</h4>

  <p>由于这个LoRA的adaptor是原来语言模型拓展出来的，所以可以按照计算LoRA的Memory saving来计算这个Adaptor节省的内存。比如，按照LoRA的公式：
\(\begin{equation}
h = W_0x + \Delta Wx = W_0x + BAx
\end{equation}\)
原本的权重$\Delta W$可以被拆解为两个低秩矩阵的乘积，如果两个低秩矩阵的秩足够低，那么理论上LoRA的参数可以无限小。</p>
</blockquote>

<p><img src="/assets/images/image-20240512231401265.png" alt="image-20240512231401265" /></p>

<p>==几个疑问==</p>

<ol>
  <li>为什么要引入一个类LoRA结构的模块来处理Visual Token？个人猜测是，从训练角度，LoRA和Q-former都可以算作对齐模块，且微调参数可以控制；从实践角度，LLM已经证明了Fine-tune LoRA可以实现部分SFT的效果，所以拿来做对齐也是可以尝试的选项。</li>
</ol>

<h4 id="训练过程">训练过程</h4>

<ol>
  <li>预训练：<strong>LLM的参数全程Freeze住，调整Vision Encoder和PLoRA的参数</strong>。预训练的能力主要由训练数据保证，数据的组织主要有几个目标
    <ol>
      <li>基础语义对齐（General Semantic Alignment）：比如图片里有什么物体等，主要用Image&lt;&gt;caption数据。</li>
      <li>整体知识认知（World Knowledge Alignment）：比如图片中包含的物体和对物体一些描述（动作，语言，简介）</li>
      <li>视觉基础能力（Vision Capability Enhancement）：比如OCR，Grounding的一些能力。</li>
      <li>预训练的数据集组织：<img src="/assets/images/image-20240515171329855.png" alt="image-20240515171329855" /></li>
    </ol>
  </li>
  <li>SFT：针对下游任务组织了数据微调，主要是QA多轮对话，Instruction数据，文本图片组合等。
    <ol>
      <li>SFT的数据集组织：<img src="/assets/images/image-20240515171348500.png" alt="image-20240515171348500" /></li>
      <li>引入了<strong>10%的Internlm2</strong>的训练数据来维持Language Model的能力。</li>
    </ol>
  </li>
</ol>

<h4 id="整体效果">整体效果</h4>

<p>在7B模型的范围里，取得了一些SOTA的结果。</p>

<p><img src="/assets/images/image-20240515204314449.png" alt="image-20240515204314449" /></p>]]></content><author><name>Chengru Song</name></author><category term="[&quot;AI&quot;, &quot;AI_Algorithms&quot;]" /><category term="301-work-ai" /><summary type="html"><![CDATA[Internlm-xcomposer 2结构详解 有用指数：⭐️⭐️ 贡献程度：⭐️ 简单评价：引入了新的架构进行模态对齐，该模块的intuitive没讲清楚，没有详实的ablation分析，现有问题分析不够明确，改进不够有针对性，看上去是没想清楚的暴力尝试，比如提升分辨率可能有用？改对齐模块可能有用？最终可能是都没用，就是暴力加数据最有用。 原文地址：https://arxiv.org/pdf/2401.16420 Existing Gap 作者认为，现在的VLLM任务还有一个没怎么探索明白的方向，就是怎么做模态对齐。目前业界对齐模态时对待Visual Tokens通常有两种方法，作者认为他们各自都有弊端。 平等对待Visual Token和Language Token，这样做忽略了两个模态之间本身的巨大差异； 认为不同模态是不同的实体，这样做会增加对齐成本。 Contribution 基于以上的Gap，作者认为自己做了两点贡献。 提出了一个PLoRA的结构，把image Token经过LoRA Module转化后再和Language Token结合在一起，这样就没有简单的把Image Token和Language Token进行混合； 组织了高质量，丰富的训练数据集。 看到这里个人有两个疑问 大部分工作（LLaVA除外）都没有把Image Token和Language Token等价，都还是使用了Q-former等结构对齐过的，所以文中提到的忽略模态差异，到底是什么差异？谁在忽略差异？ 第二，即使这个差异被忽略了，==有业界共识或者实验表明，这个差异是不可忽略的吗==？LLaVA这个结构真的不好吗？如果不好为什么Benchmark呈现的结果还行？ Method 作者提出的PLoRA结构 这个PLoRA结构，把原来LM的每一层预训练的权重换成了Low Rank Adaptor \(\begin{align*} \hat{x}_t &amp;= W_0x_t + B_0 \\ \hat{x}_v &amp;= W_0x_v + W_BW_Ax_v + B_0 \\ \hat{x} &amp;= [\hat{x}_v, \hat{x}_t] \end{align*}\) [!TIP] 拓展知识 由于这个LoRA的adaptor是原来语言模型拓展出来的，所以可以按照计算LoRA的Memory saving来计算这个Adaptor节省的内存。比如，按照LoRA的公式： \(\begin{equation} h = W_0x + \Delta Wx = W_0x + BAx \end{equation}\) 原本的权重$\Delta W$可以被拆解为两个低秩矩阵的乘积，如果两个低秩矩阵的秩足够低，那么理论上LoRA的参数可以无限小。 ==几个疑问== 为什么要引入一个类LoRA结构的模块来处理Visual Token？个人猜测是，从训练角度，LoRA和Q-former都可以算作对齐模块，且微调参数可以控制；从实践角度，LLM已经证明了Fine-tune LoRA可以实现部分SFT的效果，所以拿来做对齐也是可以尝试的选项。 训练过程 预训练：LLM的参数全程Freeze住，调整Vision Encoder和PLoRA的参数。预训练的能力主要由训练数据保证，数据的组织主要有几个目标 基础语义对齐（General Semantic Alignment）：比如图片里有什么物体等，主要用Image&lt;&gt;caption数据。 整体知识认知（World Knowledge Alignment）：比如图片中包含的物体和对物体一些描述（动作，语言，简介） 视觉基础能力（Vision Capability Enhancement）：比如OCR，Grounding的一些能力。 预训练的数据集组织： SFT：针对下游任务组织了数据微调，主要是QA多轮对话，Instruction数据，文本图片组合等。 SFT的数据集组织： 引入了10%的Internlm2的训练数据来维持Language Model的能力。 整体效果 在7B模型的范围里，取得了一些SOTA的结果。]]></summary></entry><entry><title type="html">【AI】Mixed Precision Training</title><link href="http://localhost:4000/ai/ai_algorithms/2024/05/08/mixed-precision-training.html" rel="alternate" type="text/html" title="【AI】Mixed Precision Training" /><published>2024-05-08T10:40:07+08:00</published><updated>2024-05-08T10:40:07+08:00</updated><id>http://localhost:4000/ai/ai_algorithms/2024/05/08/mixed-precision-training</id><content type="html" xml:base="http://localhost:4000/ai/ai_algorithms/2024/05/08/mixed-precision-training.html"><![CDATA[<h1 id="混合精度训练">混合精度训练</h1>

<h2 id="方法提出的背景是什么">==方法提出的背景是什么？==</h2>

<p>模型参数的增加，带来了准确度上的提升，Benchmark的提升，但是训练要求的显存也显著增大。如何在模型参数变多下，<strong>减小训练内存的使用，同时保证一定的训练精度</strong>？</p>

<h2 id="必要背景知识">必要背景知识</h2>

<ol>
  <li>AdamW优化器</li>
</ol>

<p>原本的Adam优化器是AdamW的前身，它是RMSprop和Stochastic Gradient Descent (SGD) with momentum<sup id="fnref:SGD" role="doc-noteref"><a href="#fn:SGD" class="footnote" rel="footnote">1</a></sup>这两个方法的结合版。</p>

<ul>
  <li>Adam优化器如何工作</li>
</ul>

\[m_t = β_1 * m_{t-1} + (1 - β_1) * g_t \\
v_t = β_2 * v_{t-1} + (1 - β_2) * g_t^2 \\
m_\hat{t} = m_t / (1 - β_1^t) \\ 
v_\hat{t} = v_t / (1 - β_2^t) \\
θ_t = θ_{t-1} - α * m_\hat{t} / (\sqrt{v_\hat{t}} + ε)\]

<ul>
  <li>AdamW如何工作</li>
</ul>

<p>现在基本上大模型训练都使用AdamW，AdamW的核心是，<strong>L2正则化</strong></p>

<ul>
  <li>L2正则化的引入主要和过拟合相关，超多参数易导致过拟合。过拟合的==表现形式==模型学到了训练数据中的噪声，从而不能泛化到没有看到过的数据中。过拟合的==Root cause==是，<strong>模型的复杂度过高（参数过多）</strong>，导致模型能够通过调整参数学到非常细小的数据噪声。训练过程中，一些==可能的操作导致其过拟合==，分别是①数据维度：训练数据过少；特征过多；特征的维度过高；②模型维度：模型参数过多；③训练过程：训练了太多的epoch。==解决方案==是，分别可以对应到解决其中的操作问题，在大模型的场景下，①和②的问题不太存在，主要通过L2正则化的方法来惩罚权重过大的参数。</li>
</ul>

<ol>
  <li>AdamW优化器的内存占用</li>
</ol>

<p>AdamW优化器需要保存两份状态</p>

<ul>
  <li>Ranning average of gradients(和参数的数量一致)</li>
  <li>Running average of squared gradients(和参数的数量一致)</li>
</ul>

<p>加上模型自身的参数数量，所以需要存储三份状态，两份来自于优化器，一份来自于模型本身。</p>

<h2 id="混合精度的核心原理是什么">混合精度的核心原理是什么</h2>

<p>在训练时的<strong>运算尽可能使用低精度</strong>。</p>

<p>整体的架构：</p>

<p><img src="/assets/images/image-20240509213602409.png" alt="image-20240509213602409" /></p>

<p>三个核心解决办法</p>

<ol>
  <li>存储一份FP32的单精度权重作为master copy，防止权重更新时候的梯度丢失；</li>
  <li>通过放缩Loss，来保证FP16的半精度权重的丢失问题；</li>
  <li>使用半精度计算partial product但是积累成单精度结果；</li>
</ol>

<p>其中1、2和3都是在解决半精度梯度更新时候的梯度丢失问题(underflow)。</p>

<p>1就是简单的加了一个原本权重的master copy，很好理解；</p>

<p>2就是通过<strong>移位的方法</strong>来扩展FP16能够表示的范围。引入了一个Loss Function的scaling factor，简单来说就是直接在计算FP16的时候乘上一个常数，放大FP16防止精度丢失，更新完后立刻在放缩回来。比方说下面这张图，作者把(-15,15)范围的原本的FP16右移几位即可。</p>

<p><img src="/assets/images/image-20240509215250092.png" alt="image-20240509215250092" /></p>

<p>3主要是在计算矩阵乘法的时候，dot product出来的每个子项(partial product)用FP32表示，算完加法后，再存储成FP16的格式。核心思想还是在考虑了整个<strong>运算过程中可能产生哪些underflow，并处理好这些underflow</strong>。</p>

<h2 id="细节问题">细节问题</h2>

<ol>
  <li>mixed precision training的整体效果？</li>
</ol>

<p>Top-1准确度都没有明显下降，都在正常的波动范围内。</p>

<p><img src="/assets/images/image-20240510143014296.png" alt="image-20240510143014296" /></p>

<ol>
  <li>能节省多少内存？</li>
</ol>

<p>除了存储了一份模型的master copy是FP32没有变之外，其他全都是FP16的形式进行存储。计算时，占用Memory的主要是Activation，Gradient和Weights。Activation训练时占用60-80%显存，Gradients占用20-30%，Weights占用5-10%，因此整体能够节约一半的训练显存。</p>

<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:SGD" role="doc-endnote">
      <p><a href="https://towardsdatascience.com/stochastic-gradient-descent-with-momentum-a84097641a5d">Stochastic Gradient Descent with momentum</a> <a href="#fnref:SGD" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>]]></content><author><name>Chengru Song</name></author><category term="[&quot;AI&quot;, &quot;AI_Algorithms&quot;]" /><category term="301-work-ai" /><summary type="html"><![CDATA[混合精度训练 ==方法提出的背景是什么？== 模型参数的增加，带来了准确度上的提升，Benchmark的提升，但是训练要求的显存也显著增大。如何在模型参数变多下，减小训练内存的使用，同时保证一定的训练精度？ 必要背景知识 AdamW优化器 原本的Adam优化器是AdamW的前身，它是RMSprop和Stochastic Gradient Descent (SGD) with momentum1这两个方法的结合版。 Adam优化器如何工作 \[m_t = β_1 * m_{t-1} + (1 - β_1) * g_t \\ v_t = β_2 * v_{t-1} + (1 - β_2) * g_t^2 \\ m_\hat{t} = m_t / (1 - β_1^t) \\ v_\hat{t} = v_t / (1 - β_2^t) \\ θ_t = θ_{t-1} - α * m_\hat{t} / (\sqrt{v_\hat{t}} + ε)\] AdamW如何工作 现在基本上大模型训练都使用AdamW，AdamW的核心是，L2正则化 L2正则化的引入主要和过拟合相关，超多参数易导致过拟合。过拟合的==表现形式==模型学到了训练数据中的噪声，从而不能泛化到没有看到过的数据中。过拟合的==Root cause==是，模型的复杂度过高（参数过多），导致模型能够通过调整参数学到非常细小的数据噪声。训练过程中，一些==可能的操作导致其过拟合==，分别是①数据维度：训练数据过少；特征过多；特征的维度过高；②模型维度：模型参数过多；③训练过程：训练了太多的epoch。==解决方案==是，分别可以对应到解决其中的操作问题，在大模型的场景下，①和②的问题不太存在，主要通过L2正则化的方法来惩罚权重过大的参数。 AdamW优化器的内存占用 AdamW优化器需要保存两份状态 Ranning average of gradients(和参数的数量一致) Running average of squared gradients(和参数的数量一致) 加上模型自身的参数数量，所以需要存储三份状态，两份来自于优化器，一份来自于模型本身。 混合精度的核心原理是什么 在训练时的运算尽可能使用低精度。 整体的架构： 三个核心解决办法 存储一份FP32的单精度权重作为master copy，防止权重更新时候的梯度丢失； 通过放缩Loss，来保证FP16的半精度权重的丢失问题； 使用半精度计算partial product但是积累成单精度结果； 其中1、2和3都是在解决半精度梯度更新时候的梯度丢失问题(underflow)。 1就是简单的加了一个原本权重的master copy，很好理解； 2就是通过移位的方法来扩展FP16能够表示的范围。引入了一个Loss Function的scaling factor，简单来说就是直接在计算FP16的时候乘上一个常数，放大FP16防止精度丢失，更新完后立刻在放缩回来。比方说下面这张图，作者把(-15,15)范围的原本的FP16右移几位即可。 3主要是在计算矩阵乘法的时候，dot product出来的每个子项(partial product)用FP32表示，算完加法后，再存储成FP16的格式。核心思想还是在考虑了整个运算过程中可能产生哪些underflow，并处理好这些underflow。 细节问题 mixed precision training的整体效果？ Top-1准确度都没有明显下降，都在正常的波动范围内。 能节省多少内存？ 除了存储了一份模型的master copy是FP32没有变之外，其他全都是FP16的形式进行存储。计算时，占用Memory的主要是Activation，Gradient和Weights。Activation训练时占用60-80%显存，Gradients占用20-30%，Weights占用5-10%，因此整体能够节约一半的训练显存。 Stochastic Gradient Descent with momentum &#8617;]]></summary></entry><entry><title type="html">【AI】Deepspeed Training</title><link href="http://localhost:4000/ai/ai_algorithms/2024/04/28/deepspeed.html" rel="alternate" type="text/html" title="【AI】Deepspeed Training" /><published>2024-04-28T15:04:07+08:00</published><updated>2024-04-28T15:04:07+08:00</updated><id>http://localhost:4000/ai/ai_algorithms/2024/04/28/deepspeed</id><content type="html" xml:base="http://localhost:4000/ai/ai_algorithms/2024/04/28/deepspeed.html"><![CDATA[<h1 id="deepspeed">DeepSpeed</h1>

<h2 id="这篇文章主要回答">这篇文章主要回答</h2>

<ol>
  <li>DeepSpeed要解决什么问题？</li>
  <li>DeepSpeed如何解决的问题？</li>
  <li>如何部署DeepSpeed进行训练？</li>
  <li>相比于其他训练方式DeepSpeed有什么优势？</li>
  <li>还有哪些训练框架？</li>
</ol>

<h2 id="deepspeed要解决什么问题">DeepSpeed要解决什么问题</h2>

<p>为了解决千亿甚至万亿大模型的训练问题，因为这种大模型训练通常需要占用巨大的显卡内存，因此很可能拥有的设备根本训练不起来，即使训练起来了，也可能速度很慢。</p>

<p>如何对训练的效率进行衡量？</p>

<h3 id="训练的内存占用如何计算">训练的内存占用如何计算</h3>

<p>1.5B参数量的大模型，如果精度是FP16(单个参数占用2bytes)，则模型内存占用为2x1.5=3B，如果用Adam Optimizer + 混合精度训练<sup id="fnref:1" role="doc-noteref"><a href="#fn:1" class="footnote" rel="footnote">1</a></sup>，模型存储自身参数+梯度，就变成了3B+3B=6B，混合精度训练相加时，又需要FP32的拷贝（momentum+variance）即4x1.5（原模型参数）+4x1.5（momentum) + 4x1.5(variance)=12B，加上自身参数和梯度，共16x1.5B=24G。所以1.5B参数量的模型，训练就需要24G内存。</p>

<h3 id="训练速度如何衡量">训练速度如何衡量</h3>

<p>FLOPS来衡量，就是每秒可进行的浮点数计算。</p>

<h2 id="deepspeed如何解决问题">DeepSpeed如何解决问题</h2>

<h3 id="常见优化手段">常见优化手段</h3>

<ul>
  <li>Data Parallelism（数据并行）：每个GPU都存储一个模型，不会减少每个GPU的Memory使用，只能提升训练速度；</li>
  <li>Pipeline Parallelism（流水线并行）：一个GPU装不下一个模型，但装得下一层或者多层，因此把同一个模型拆开训练；</li>
  <li>Tensor Parallelism（张量并行）：每个张量被拆分成多个块,因此不是整个张量驻留在单个GPU上,而是张量的每个分片驻留在指定的GPU上。在处理过程中,每个分片都在不同的GPU上单独并行处理,结果在步骤结束时同步。</li>
</ul>

<p>流水线并行和模型并行的概念似乎是等价的，好处是能开起来训练了，坏处是训练速度和通信强相关，会显著变慢。</p>

<h3 id="deepspeed的优化手段">DeepSpeed的优化手段</h3>

<p>DeepSpeed主要关注在Data Parallelism，这样不存在模型并行时候存在的更新通信速度问题。在解决DP问题的时候，DeepSpeed主要是把Model states进行<strong>分片</strong>，而不是<strong>复制</strong>。</p>

<p>DeepSpeed共有三个级别，分别能实现三种级别的内存效率。</p>

<ul>
  <li>一级：优化器状态分片</li>
  <li>二级：梯度分片</li>
  <li>三级：参数分片</li>
</ul>

<p><img src="/assets/images/image-20240428173117885.png" alt="image-20240428173117885" /></p>

<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:1" role="doc-endnote">
      <p><a href="https://towardsdatascience.com/understanding-mixed-precision-training-4b246679c7c4">Understanding Mixed Precision Training</a> <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>]]></content><author><name>Chengru Song</name></author><category term="[&quot;AI&quot;, &quot;AI_Algorithms&quot;]" /><category term="301-work-ai" /><summary type="html"><![CDATA[DeepSpeed 这篇文章主要回答 DeepSpeed要解决什么问题？ DeepSpeed如何解决的问题？ 如何部署DeepSpeed进行训练？ 相比于其他训练方式DeepSpeed有什么优势？ 还有哪些训练框架？ DeepSpeed要解决什么问题 为了解决千亿甚至万亿大模型的训练问题，因为这种大模型训练通常需要占用巨大的显卡内存，因此很可能拥有的设备根本训练不起来，即使训练起来了，也可能速度很慢。 如何对训练的效率进行衡量？ 训练的内存占用如何计算 1.5B参数量的大模型，如果精度是FP16(单个参数占用2bytes)，则模型内存占用为2x1.5=3B，如果用Adam Optimizer + 混合精度训练1，模型存储自身参数+梯度，就变成了3B+3B=6B，混合精度训练相加时，又需要FP32的拷贝（momentum+variance）即4x1.5（原模型参数）+4x1.5（momentum) + 4x1.5(variance)=12B，加上自身参数和梯度，共16x1.5B=24G。所以1.5B参数量的模型，训练就需要24G内存。 训练速度如何衡量 FLOPS来衡量，就是每秒可进行的浮点数计算。 DeepSpeed如何解决问题 常见优化手段 Data Parallelism（数据并行）：每个GPU都存储一个模型，不会减少每个GPU的Memory使用，只能提升训练速度； Pipeline Parallelism（流水线并行）：一个GPU装不下一个模型，但装得下一层或者多层，因此把同一个模型拆开训练； Tensor Parallelism（张量并行）：每个张量被拆分成多个块,因此不是整个张量驻留在单个GPU上,而是张量的每个分片驻留在指定的GPU上。在处理过程中,每个分片都在不同的GPU上单独并行处理,结果在步骤结束时同步。 流水线并行和模型并行的概念似乎是等价的，好处是能开起来训练了，坏处是训练速度和通信强相关，会显著变慢。 DeepSpeed的优化手段 DeepSpeed主要关注在Data Parallelism，这样不存在模型并行时候存在的更新通信速度问题。在解决DP问题的时候，DeepSpeed主要是把Model states进行分片，而不是复制。 DeepSpeed共有三个级别，分别能实现三种级别的内存效率。 一级：优化器状态分片 二级：梯度分片 三级：参数分片 Understanding Mixed Precision Training &#8617;]]></summary></entry></feed>