---
layout: article
title: 【Blog】Deepseek R1 Sharing
aside:
  toc: true
key: work_reflection
date: 2025-02-07 12:30:07 +0800
tags:
- 301-work-blog
category: [work, Blog]
typora-root-url: ../../../blog
---

# Deepseek R1 Sharing

## TLDR

1. 打破LLM训练从Generalist到Reasoner的常规思路，使用RL先得到Reasoner，再经过SFT成为Generalist；
2. GRPO算法的高效性，让RL大规模训练取得了效果；
3. SFT with CoT数据的训练结果在原文中并未给出，但从最近一些复现工作和R1 distilled Qwen结果来看，SFT的作用可能比RL还大。
4. 好的Base模型本身就有Aha Moment，但是是Superfacial的reflection，不具有提升Accuracy的能力，RL可以增强该能力。

## R1-Zero是如何成为Reasoner的？

一句话：大基座（671B MoE模型） + Rule-based Reward + GRPO RL算法。

1. 大基座，基座是Deepseek-V3-Base模型MoE模型，在大基座模型上观测到了Reasoning的涌现能力（Aha Moment）
2. GRPO算法的高效性。在LLM语境下，PPO算法（On-policy RL）一般是这样实现的
   1. Policy Model：Base LLM
   2. Value Function：Reward Model，给一条回复的打分，一般从同Size的模型训练得到；
   3. Advantage Function：对Reward Model的结果进行GAE，得到每个动作（每个Token）的Advantage；
   4. Importance sampling：对比新Advantage和原有policy的比值，只对Advantage进行小比例更新，防止模型训崩。
   5. Gradient update：对policy Model进行梯度更新。
   6. 
3. 原始PPO存在几点问题
   1. 需要一个等大小的模型作为Reward Model，训练速度慢；
   2. 需要借助外部模型作为Value Function，Value Function的训练是否稳定，效果评估也存在问题；
   3. 对于Rule-based Reward，该算法需要一定的改进。
4. GRPO怎么做
   1. 核心在于如何把Reward（Value Function）转化为给每个动作（token生成）的Advantage
   2. 改进一：每个问题进行G次回复，组成分组，组G中有G个奖励$$r=\{r_1, r_2, ..., r_g\}$$，直接把Advantage Function定义为Reward的标准值，即 $$A_t^i=\frac{r_i-mean(r)}{std(r)}$$.
   3. 改进二：GRPO直接通过策略模型和参考模型的KL散度优化作为更新步长（惩罚项），简化了训练过程。

## R1-Zero都做了哪些事情？

1. 从Deepseek-V3-base开始，使用GRPO算法对模型进行RL训练，使用以下两个Rule-based Reward
   1. Accuracy Reward：最终结果的Format是否正确，Leetcode题能否过编译，和最终答案是否一致；
   2. Format Reward：是否把Thinking过程包在了`<think>` tag中。

结果：观测到了Test-time scaling，Accuracy在上涨。

Accuracy随训练步数在增大

![img](/Users/bytedance/Documents/blog/assets/images/(null)-20250208172734142.(null))![img](/Users/bytedance/Documents/blog/assets/images/(null)-20250208172733751.(null))

Thinking time随训练步数在增大

## R1做了哪些事情

R1的训练过程如下

1. **SFT冷启动**：使用**数千条**（具体数量文中没有提及）训练Deepseek-V3-Base，这几千条数据的特征是，few-shot prompting with a long CoT，使用Reflection和Verification生成。主要提升**可读性和RL潜力**。
2. **Reasoning RL训练**：使用RL对有明确反馈信号的任务进行训练，主要是Coding，math，science和logic reasoning。因为这些问题是**有明确对错Reward**的任务。通过Long CoT解决。**这步没有Format Reward！**没有Format Reward原因也很简单，SFT之后不太会有Format问题。有的Reward是两个，①对错Reward；②语言一致性Reward（语言不一致给个地方）。直接相加作为最终Reward。
3. **拒绝采样SFT**：数据两部分组成，reasoning+Knowledge。Reasoning扩展数据量级到**600k**，任务从之前明确对错问题**扩展到通用任务**。数据用第2步的ckpt过滤了CoT有问题的数据+Deepseek-V3的生成式Reward模型过滤了答案有问题的数据，最终生成的数据对模型做了SFT。Knowledge数据量级**200k**，但对于其中部分数据（量级不详）构造了CoT，**但SFT是从Deepseek-V3-base开始的，并不是从第2步的ckpt开始的！**
4. **全量RL**：reasoning数据继续使用rule-based Reward，通用类回答复用了Deepseek-V3的Pipeline构造出了同分布的preference pairs，还有部分通用preference alignment数据，进行全量RL训练。

## R1技术迭代路径是什么？

考虑到R1-zero的一些设计和最终R1的结果，我推测过程可能如下：

1. Deepseek想最开始直接做纯RL的模型，想办法提升模型的Reasoning能力，就提出了R1-Zero，用Format Reward + Accuracy Reward直接训练出来一个模型；
2. 随后发现出现了Aha moment，模型能自己reason了，但是说话能力比较差；常见的提升说话能力的就是直接SFT，做Next token prediction训练；
3. 随后搞了RL + SFT，大概率同时发生了两件事，①说话能力提升明显，但是reasoning能力下降更明显；②保持住部分reasoning能力，但是说话能力救不回来了。
4. 后面再训练的目标非常明确：**保住Reasoning Aha moment的同时，让模型能正常说话**。所以干脆把SFT放到前面，但是只用少量数据，先保证模型能正常说话，再训练Reasoning能力，相当于回到之前的工作流里面，但由于第一步已经有了Aha moment，第二步RL的数据是不需要再组织的。
5. 个人猜测这两步之后的流程就是想办法加入通用数据，把其他任务的Benchmark刷的好看点，加上有之前SFT时候构造Reasoning SFT的经验，直接把这个复用到通用任务上。但我觉得这步的耗时可能非常多，因为这是一个最后一公里任务，还是比较难的。
6. 最后一步RL是通用的，任何模型都会做的事情，但由于之前的RL已经产生了Aha moment，给模型的CoT带来了泛化性，所以在其他通用任务上，这个CoT + SFT出现了奇效。
7. 引用下上篇博文的其他大佬的总结：这个工作本质验证的是，让一个Reasoner变成Generalist，而不是反过来。但启动得用500B+基座。但是否完全需要500B+的基座启动R1-zero，也不一定。因为这几天已经有很多复现工作了，用的也不是500B+的大模型，也都是10B以下的小模型，所以让子弹再飞一会。

## 没透露的技术细节是什么？

### RL作用的粒度

![img](/Users/bytedance/Documents/blog/assets/images/(null))

如果只作用于结果，而不作用于reasoning，是没有理由画出来第8页的推理时间随训练时间逐渐增长的曲线的。

所以，训练时候reward一定鼓励了模型增加自己的reasoning的长度，但不一定是通过设计Reward Model实现的，也可能是用数据实现的。比如，GRPO中这个group的outputs到底是什么，是纯policy model生成的几个，还是做了一些策略，比如把多步reasoning和单步reasoning都放到一个group里面，只是思考时间长的正确率高，所以训练会favor更长reasoning。

### SFT的数据设计

这个非常关键，有几个观察可以佐证。

1. 人工检查：我看了下回答，开放式回答的format一致性是非常高的，都是分步，分点回答的，说明SFT的Format是经过了设计的。

![img](/Users/bytedance/Documents/blog/assets/images/(null)-20250208172734183.(null))

1. Distillation结果，直接使用R1蒸馏出来的数据，对其他开源模型进行SFT，其他开源模型的结果都变好了，进一步验证了，SFT对R1实际表现很重要；
   1. ![img](/Users/bytedance/Documents/blog/assets/images/(null)-20250208172733753.(null))
2. Qwen-32B-zero的结果，在Qwen-32B小模型上直接用RL方法，而不经过任何SFT，效果跟R1-zero差不多，但R1蒸馏数据SFT之后的Qwen-32B效果显著提升。**说明SFT的作用，可能比RL更管用。**考虑到技术报告只给了这一个和SFT，RL的ablation实验，而没有给出R1-zero和R1更详细的对比，这里面有猫腻的可能性非常大。

### 一些其他思考

1. 奥卡姆剃刀！用第一性原理分析下，我倾向于相信，模型是用简单的方法做出来的，因为越复杂的Reward会导致模型朝着过拟合的方向发展，尤其是作者做了其他RL方法实验，MCTS和Process Reward都不work，更是说明了这点。***但R1和R1-zero的Reward一定是不同的***，因为，R1-zero没经过任何SFT冷启动，Format大概率是不对的，所以Format Reward的设计是合理的。但R1是有SFT冷启动的，Format出错概率很小。因此，可能R1的Reward是Align`<think>`之外，还要求Align SFT结果的Format（比如分步作答，全面思考）。或者，R1的Reward干脆就没有Format Reward，能力都是SFT带来的。但我倾向于相信是有的，因为现有模型都没有Deepseek能力强，不可能凭空跑出来一批SFT数据。同时，R1的RL和SFT可能是同一批数据，这样能保证两者训练完是能对齐的。
2. 社会工程学角度和贝叶斯概率：Deepseek说破大天就200多人做这件事，不可能比2000人团队做出来更多的实验，可能少一个人就能少10个消融实验，所以不太可能是特别复杂的方法。更可能是简单方法，但是做对了很多中间结果，汇总成了最终结果。

## 有什么试图复现的工作？效果如何？

复现的定义是什么？

在某个大小的模型上，观测到**Self-verification和Search solution space的涌现能力**。同时在任务上的Score随着训练准确率提升。其中一个观测指标是，回复长度随时间增加的Scaling曲线，如Deepseek放出来的。

1. TinyZero：https://github.com/Jiayi-Pan/TinyZero by Jiayi Pan(Standford PhD)
   1. Key insights：
      1. 在Instruct模型上跑出来了thinking和steps的scaling，而且Instruct模型的训练过程明显更稳定。**说明SFT冷启动非常重要！**
         1. ![img](/Users/bytedance/Documents/blog/assets/images/(null)-20250208172733729.(null))
      2. 一点遗憾：Thinking的scaling没带来结果上的变好，validation更差了：
   2. 模型大小：**Qwen2.5-0.5B模型和Qwen2.5-3B**
   3. 场景：24点游戏，给定四个数字和结果，通过四则运算得到该结果。
   4. 结论：
      1. 有CoT过程，有Self Reflection和Solution space Search，但是没有看出来涌现能力。
      2. 有借鉴意义，但24点游戏对于RL是非常简单的任务，不通过R1-Zero的reasoning也能做出来，但另一个侧面想，小模型在小任务上能跑出来已经是了不起的成就了，Scale模型也许就能在更多任务上跑出更好的结果。
2. GRPO demo by Will Brown
   1. Key insights: 使用好的System prompt + 较低KL coef
   2. 结论：使用GRPO，在GSM8K上相比原生从41.6%提升到了51%。但是没有提直接SFT带来的提升。
   3. Reward：使用了int Reward + Format Reward  + correctness Reward。
3. SFT Memorizes, RL Generalizes (Tianzhe Chu HKU, ): https://tianzhechu.com/SFTvsRL/
   1. Key insights: RL在规则泛化性上更好（但没有说明是CoT带来的，文中也没有任何跟CoT有关的讨论，应该是R1之前就在写的文章，只是R1出来后发出来了）。
      1. 训练scaling曲线
      2. ![img](/Users/bytedance/Documents/blog/assets/images/(null)-20250208172734375.(null))
   2. 模型大小：LLama-3.2-Vision-11B。
   3. 作者Design了两个场景，一个是纯文本的`General Points`，通过在Inference时候临时改变规则，观测模型的泛化性（比如JQK是代表11,12,13还是全都是10）。另一个是Vision场景的辨别，在地图上把原先东北转向变成Slight Right，看模型还能否做对。
   4. 讨论
      1. 24点改规则的游戏，用了RL还是低的离谱。Acc从11.5%到15%，SFT从11.5%掉到了3.4%。要知道不用LLM，纯RL做24点都比这个效果好。
      2. Vision任务，不如改规则的说服力强，原因是语言模型原本可能包含对northeast和Slight Right的语义理解，经过SFT之后强化了northeast，丢失了语义能力。而且SFT之后，OOD还不如Baseline（80.8%->1.3%），大概率说明的是SFT有问题，还谈不上SFT比不上RL。
      3. 从理论角度讲，NTP loss能训练成一个能做题的或者做好一个环境互动任务的本来就不make sense，但只要建模没问题，RL能做好就是较为正常的事情。所以文章的说服力并不是很强。
4. OpenR1：https://huggingface.co/blog/open-r1
   1. 目标：补齐R1 Pipeline里没讲清楚的事情，完全复现R1的结果
   2. 预期动作
      1. 复现R1蒸馏模型：从R1蒸馏等量SFT数据，在Qwen模型上Finetune，取得和R1 Tech report里面用SFT蒸馏的Qwen相同指标；
      2. 复现R1-zero：用Math，reasoning和code数据训练得到R1-Zero。
      3. 复现R1：根据R1-Zero的复现结果生成Reasoning data，再复现完整的R1 Pipeline。
      4. ![img](/Users/bytedance/Documents/blog/assets/images/(null)-20250208172733615.(null))
   3. 评价
      1. OpenR1是一个比较严肃的，试图复现R1完整工作的流程，是一个复杂度非常高，难度很大的项目。个人认为能够做到Open R1 Zero就已经成功了，R1能达到现有水平的80%水位就是完美。一个很重要的原因是SFT阶段的数据不太好做，Deepseek内部应该做了非常多工作，想要低成本打平数据上的Gap是很难的。光R1-Zero里面的数据，需要每一条都验证正确性，Coding的还要过编译器，组织这些数据已经是非常非常难的工作了。
5. 李飞飞最新模型S1：s1: Simple test-time scaling
   1. TLDR：在MATH和AIME24的Benchmark上，通过在**1k数据上纯SFT with CoT trace**带来了27%的提升。并且**观测到了Test-time scaling现象。**并且数据不能多了，如果把所有SFT with CoT Trace都用上，效果反而很差。
      1. ![img](/Users/bytedance/Documents/blog/assets/images/(null)-20250208172734009.(null))
      2. 这进一步说明了RL和SFT with CoT trace在模型推理能力的提升上消融实验做的还不够，认知有待继续提升。
   2. 目标：尝试通过一些手段实现Test time scaling。
   3. 做了什么：在**Qwen2.5-32B-Instruct**模型上，用**1K**根据规则筛选出的**有CoT Trace的SFT数据**进行SFT训练，并在Test time时候用了以下Decoding strategy
      1. 如果Thinking过长：在设定好的截断长度强行加入`Final Answer：`，让模型结束思考开始回答；
      2. 如果Thinking过短，改`Final Answer`成`Wait`，让模型继续思考。
   4. 1K数据如何构造
      1. 59K初始数据构造：NuminaMATH（30k），AIME，Olympic Arena（4.2k），Omnimath（4.2k），AGIEval（2.3k)，SAT。
      2. 构造数据的reasoning Trace：调用google Gemini Flash Thinking API。
      3. 过滤到1k数据
         1. Quality过滤：Format错误。
         2. Difficulty过滤：Gemini回答正确且Thinking length > 5600
         3. Diversity过滤：遍历所有domain，按照Thinking length倒排，并按照2的负倒数做weighted sampling（所以更倾向于Thinking比较长的）
   5. 结果如何
      1. s1-32B已经远比原本的Instruct版本好了，虽然还是没有追上R1蒸馏出来的模型（which用了800k SFT数据）
      2. ![img](/Users/bytedance/Documents/blog/assets/images/(null)-20250208172734371.(null))
   6. 讨论
      1. SFT和RL分别的作用：在原始R1的paper里，Deepseek并没有提及SFT和RL分别给R1目前的能力带来的提升幅度，也没有讲最开始冷启动SFT后模型的各项指标参数，这个文章在该方向给出了一些insights，有一定参考价值。
      2. 还有另一篇重量级notion，讲了为什么可能R1根本没有Aha Moment，只是Base训练的好而已。这也是为什么只有在某些基座上能训练出来（比如Qwen），但是其他基座不行的原因，甚至和Size的关系都不大。
6. There May Not be Aha Moment in R1-Zero-like Training — A Pilot Study
   1. Base模型，用上以下template，在没有任何训练的情况下，模型已经开始Self reflection了，其中R1和Qwen family模型效果尤其好，不需要RL已经出现了Aha Moment
   2. ![img](/Users/bytedance/Documents/blog/assets/images/(null)-20250208172734253.(null))
   3. https://oatllm.notion.site/oat-zero

## 如何应用到当前业务场景或自研模型建设上？

1. 目前一些不太严肃的复现工作，例如TinyZero，或者其他纯基于GRPO+Format Reward，Accuracy Reward，用RL不太能观测到Test time scaling；
2. 而李飞飞最近的工作证明了，SFT with CoT Trace可能是就直接效果非常好了。
3. 同时另一个博客提到，RL的Aha Moment可能并不存在，纯因为Base模型比较好，有一些reasoning的pretrain数据带来的。

综上所述，一个比较快速的方法是，直接用R1在业务场景下蒸馏CoT数据，然后直接对基座较好的模型（例如Qwen-Base）进行SFT，就能得到一个专有领域带reasoning的小模型。

## 部分背景知识

### Deepseek-V3 Pipeline

TBD

## Reference