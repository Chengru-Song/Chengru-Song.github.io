---
layout: article
title: 【Blog】2024大模型年终总结
aside:
  toc: true
key: work_reflection
date: 2025-01-04 13:46:07 +0800
tags:
- 301-work-blog
category: [work, Blog]
typora-root-url: ../../../blog
---

# 2024 大模型年终复盘

## 个人情况

1. 个人情况：我是业务部门的大模型算法，主要负责在业务场景下应用大模型技术，优化之前的业务流或者实现新的业务功能。
2. 工作职责：
   1. 从大模型视角设计全套技术解决方案，不止包含大模型本身。比如模型本身是多模态或者单模态大模型，我的工作需要考虑系统的**业务流程，设计数据流、模型组合方案、模型训练、serving策略、结果评估**等。
3. 从业时间：从其他算法转行大模型时间1年2个月。

## 工作总结

1. 效果评估：以终为始
   1. 如果我能重来一次，我肯定要先和业务方明确下游任务类型，并确定非常详细的评估标准，比如幻觉、准确度、多样性等业务指标。如果是美学标准，可以定义美学分数。然后以最快的速度拉齐一个Benchmark数据集。这个Benchmark就是目标，所有优化都应该向着这个目标进发。
   1. Benchmark即结果：制定Benchmark看似比较业务，和模型训练都没什么关系，但是这个Benchmark可以不仅用于标注训练数据，也方便在实验后复盘为什么某些指标没达到，到底是数据的问题还是训练过程的问题还是模型架构的问题。如果Benchmark上模型已经取得了SOTA，那么就应该design新的Benchmark，新的Benchmark往往来自于产品的迭代，产品独创新玩法，大模型在新的业务场景落地等。这时候要自己去“找食吃”，找到更多的业务方合作，或者推动产品创新，否则很难证明自己的价值。
2. 数据构建：自动化数据流水线
   1. 定时自动化更新训练数据集：不管对于基础模型的训练还是业务演进都非常重要，做大模型大家一般都有一些共识，就是模型成功的核心在于数据，懂行的老板们对数据一般都追得比较细。一来是因为Transformers已经被证明是一个很高效且scalable的架构，堆数据和算力大概率是有用的，二来是因为有很多研究也表明，数据量级的提升确实对模型能力的提升有正向作用。因此定期的可用于标注的数据量级的变化，很大程度上就决定了模型能力的下限。
   2. 定期的自动化和人工数据标注：能定时积累大量数据并非全部，数据质量的影响可能远大于数据数量。因此需要建立一定的高质量数据标注流水线，自动化手段也是可以接受的，有人工参与的自然更好。
3. 模型设计：结合业务反馈设计模型
   1. 业务反馈作为Reward Model独立于基础模型：现有的生成式模型，生文或者生图类的，基本上都是以Next token的交叉熵或者重建Pixel损失作为目标，在实际业务应用中，纯生成质量上的评估体系不一定能对齐到真实业务目标，而显式拆解业务到质量维度又可能会造成离线指标和线上指标无法对齐的情况。因此，将业务设计成某种Reward Model，对生成结果进行反馈，再影响到模型训练，是一种很好的建模方式。
   2. 把业务作为特征加入基础模型：做过多模态大模型（VLM）的可能了解，VLM的一个主要思路，就是把非文本模态的特征（例如ViT特征）投射到LLM，从而给LLM补充了视觉能力。因此，业务侧的Dense特征，可以类比ViT特征，通过多层MLP投射成LLM embedding，指导LLM生成。该方案的难点在于，业务侧特征相比ViT可能不具有任何语义，因此训练过程是否稳定，能否收敛，都依赖于该特征和文本之间的关联关系是否显著。实际上不光业务特征，很多模型架构（例如Qwen-Flux）也都借鉴了类似的思路。
4. 模型训练：关注开源和Inhouse指标变化
   1. 独立设计Pretrain和post-train指标：如果模型在业务上有瓶颈，例如某个幻觉情况无法改善，大概率需要在Pretrain阶段发力。
   2. Post-train持续迭代和优化数据混合：①数据量不需要多，一般来说post-train有几十k到上百k数据足够，参考Deepseek-R1，在post-train阶段不过使用了800k数据集；②一定要混合开源数据，业务数据需要混合高质量开源数据，否则不利于业务scale，比如微调后模型改prompt可能不Work；③数据混合比例需要不断调整，根据训练完成的Benchmark调试数据混合比例，在最佳比例上做Data Scale。
   3. 