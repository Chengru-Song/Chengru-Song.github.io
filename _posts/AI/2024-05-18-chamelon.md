---
layout: article
title: 【AI】Chameleon - Meta全模态大模型
sidebar:
  nav: AI
aside:
  toc: true
key: llm
date: 2024-05-18 11:21:07 +0800
tags:
- 301-work-ai
category: [AI, AI_Algorithms]
typora-root-url: ../../../blog
mermaid: true
---

# Chameleon Meta全模态大模型

> - 时间：2024.5.17
> - 作者团队：FAIR at Meta（Meta的AI研究团队）
> - 作者：
> - 有用指数：⭐️⭐️⭐️⭐️⭐️
> - 贡献程度：⭐️⭐️⭐️
> - 简单评价：

多模态大模型，常规来看有两个思路。第一是之前流行的，把==视觉模块对齐到语言模块==，一般会用到一个预训练的视觉特征提取模型和一个语言模型，中间用一些全连接或者Q-former结果连接，使用一些图像和文本对数据集进行对齐；第二种是全模态统一大模型，神经网络的输入和输出都使用相同的tokenizer，用一个Transformer的结构来处理这些全模态token，并输出多模态token来解码。

第二种显然是大家正在发力的方向。例如今天的这篇文章，Chameleon。GPT-4o也是类似的架构，只是GPT-4o的模态会更全一些，包含TTS的token。

## Existing Gap

分开对待语言和视觉模块是严重限制了模型输出多模态的结果。

## Contribution

1. 把文本和图像的tokenizer进行了统一，方法是把图片量化成离散的token。

这里我有一个疑问

*文本的tokenizatioin比较好理解，因为语料库的大小是有限的，所有单词或者token的数量也就几万，组成复杂文本的就只有这几万个基本元素。但组成图片的基本元素有哪些？图像的tokenization该怎么做？*

## Method

1. 场景评估：作者想focus在三个场景，纯文本，单图文pair和图文interleaved形式
2. 图像Tokenizer选取：

图像Tokenizer的选择需要了解之前Meta的一个工作：[Make-A-Scene: Scene-Based Text-to-Image Generation with Human Priors](https://arxiv.org/pdf/2203.13131)，简单介绍下。



