---
layout: article
title: 【AI】看完Seed-1.5-VL技术报告，立刻可尝试的几件事
sidebar:
  nav: AI
aside:
  toc: true
key: llm
date: 2025-05-13 13:11:07 +0800
tags:
- 301-work-ai
category: [AI, AI_Algorithms, Vision]
typora-root-url: ../../../blog
mermaid: true
---

# 看完Seed-1.5-VL，立刻要尝试的一些操作

## 结论

> - 单独Pretrain ViT模块
>   - **ViT蒸馏**：使用大的CLIP模型蒸馏一个小CLIP模型，直接用用两个模型的output features的cos距离作为loss，能极大提升模型的文档理解和OCR能力；
>   - **ViT Pretrain with 2D RoPE**：使用SigLip loss和SuperClass loss 对ViT进行Pretrain；
>   - **全模态ViT Pretrain**：提到了MiCo的训练框架，即把视频，音频，depth信息都用ViT统一Encode，和文本Encoder进行contrastive learning
> - Pretrain
>   - **数据去重和Domain Balance**：限制common Domains的数据数量在某个最大值，会提升综合检出率；
>   - **OCR数据加强**：修改图片，例如模糊一下，增强模型鲁棒性；
>   - **Chart数据、Table数据反向合成**：直接用LLM写python code生成chart图表，再把生成code作为输出反向训练；Table的用HTML等渲染；
>   - **Grounding和Counting**：加bounding box后用现成VLM过滤错误标注，外加主体识别后用CLIP过滤部分数据；counting的数据使用了相对坐标，并normalize相对坐标到[0,999]这个范围，从而解决了定位问题；
>   - **3D 位置理解**：DepthAnything V2推理位置信息，生成数据；
>   - **Video temperal和grounding数据**：这部分数据能在复杂视频任务上发挥很大的作用；
> - **三阶段训练**：MLP only；All with OCR & visual Grounding; All but high quality and more context length.
> - **Scaling Law**：各个子分类下数据token量级和Metrics显示出了log Linear的关联关系；
> - **Post-training**
>   - **SFT**：
>     - **General instruction**：只训练Complex instruction，但是平衡了任务类别下的数据；
>     - **Reject Sampling**，用Reward Model只选出来30k Sample
>   - **Reward Model**
>     - 人工标注评分数据，数据是通过原始模型Sample出来的，并且每个Response内部过滤了下，只保留差别较大的回答；
>     - 合成数据：找有GT的数据，设置一个format，ranking按照回答+format正确，format正确或回答正确，format错误组成排序list。
>   - **RLHF**
>     - prompt分布会显著影响效果，所以流程是prompt能力分类->分层采样->Reward score方差筛选（保留方差较大的，Reward Model可区分好坏）->简单prompt下采样。
>   - **RLVR**
>     - 设计各种不同任务的Reward，比如STEM->Acc, Grounding->IOU，IF->regex, Puzzle -> verifier
>   - **Iterative update**
>     - RL和SFT混着来，SFT跑完走RL，RL完了再更新数据跑SFT，跑了四轮。
