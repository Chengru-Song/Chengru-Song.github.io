---
layout: article
title: GBDT(Gradient Boosting Decision Tree)详解
sidebar:
  nav: AI
aside:
  toc: true
key: gbdt
tags:
- 301-work-ai
- 301-work-basics
- 301-work-interview
category: [AI, AI_Algorithms]
---
# GBDT

Advantage: The advantage of GBDT is that it achieve great performance. 
对数据特征尺度不敏感，自动填补确实特征，可做特征筛选，效果较为突出。
Key word: Regressor
Usage: A regressor to forecast different labels or values

# Great Explanation

[GBDT Algorithms: Principles - Develop Paper](https://developpaper.com/gbdt-algorithms-principles/)

Watch chapter 3 for formulation

[Adobe Acrobat](https://documentcloud.adobe.com/link/review?uri=urn:aaid:scds:US:57ad387f-d35d-422e-9961-8bc8fba7b602)

# Core idea

Whether weak learners could be modified to become better. 

# Logic Flow

## Decision trees

Decision trees can be modeled like this:

![Image](/assets/images/gbdt.png)

By constantly switching from different decisions, finally output something that help make decisions, let's say forecast. 

## Ensemble learning

Using decision trees is a way to make decisions by calculating the residual of each leaf and update the prediction result. However, using a decision tree may overfit rather quickly and lack the ability of generalization. As a result, they want to combine lots of decision trees for learning. 

### Random forest

One way is to use random forest that combines the weighted sum of prediction accuracy of different decision trees. This could be run in parallel. But not weighted sums

## Gradient boosting decision trees

Combine many weak learners to form a strong learner. The way it forms a strong learner could be seen as the following.

![Image](/assets/images/gbdt_1.png)

The process of updating the decision trees could be described as the following.

1. Using loss function and gradient descent method with first decision tree. 
2. Error residuals for first decision tree: e1 = y - y_prediction1, which is e1_predicted.
3. Prediction for second decision tree is y_prediction2 = y_prediction1 + e1_predicted. 
4. Residual of the second decision tree is e2 = y - y_prediction2. which is e2_predicted. 
5. Repeat the process and update the residuals sequentially. 

Following the previously mentioned procedures, reach a final state and terminates.