---
layout: article
title: 【AI】RL的本质
sidebar:
  nav: AI
aside:
  toc: true
key: llm
date: 2025-10-30 17:35:07 -0700
tags:
- 301-work-ai
category: [AI, AI_Algorithms, Vision]
typora-root-url: ../../../blog
mermaid: true
---

# 从Post-Train角度思考VLM模型变化，应用发展

## 前言

上个季度开源release了非常多评分很高的模型，比如Qwen3-VL，Ovis-2.5，Mimo-VL等，这些模型本身的能力得到了非常大幅度的提升，在数学和推理类的任务上提升非常显著。作为从业者，我其实不禁会思考两个问题

1. 对于模型Reasoning训练，最高效的方法到底是什么？
2. 这些模型的应用场景在哪里，从业者有什么机会？

## 模型能力的思考

对于Late fusion类的VLM来说，目前架构上的创新其实不多，或者更直白一点，有能力做架构创新的，不太多。核心原因就是算力。一个标准的Late fusion VLM由三部分组成，ViT、connector和LLM。融合的方式从比较简单的LLaVA系列，直接MLP做connection，到最新的Qwen-VL系列，通过Deepstack更深度融合ViT的Feature（其实是有点LLaMA adapter的文艺复兴）+ 文本标注视频帧，还有一些做Vision token的方案，例如Ovis。

这些方法不仅是架构的简单到难，更是算力的从少到多。LLaVA最开始的架构只需要150k就可以完成训练，且更新的参数较少，不需要更大的训练资源。资源紧缺情况下，甚至可以只训练connector，都不需要打开ViT和LLM的参数。

而改完架构之后想做好深度融合，肯定少不了大量的训练，从ViT到LLM都需要专门的优化。很多人可能试过，如果把Qwen本身的ViT替换掉，训练不充分的话，无论如何也训练不出来Qwen本身的效果。

所以，模型能力提升，除了基模Pretrain和Post-train组，有千卡万卡资源，大部分算法优化模型效率最高的还是数据、训练方法或者直接做应用。如果做应用算法的话，我们至少应该了解，哪些应用场景比较适合落地，我们又该怎么准备？

## VLM应用的思考

模型应用一直是基础模型的老大难，尤其是VLM；LLM最大的落地目前是coding和chatbot，VLM落地很多大组织，包括我个人都认为，这些方向就包含具身智能，Browser Use和computer use等，但browser use又是一个更基础的能力，作为具身智能的一部分存在。

为什么呢？作为从业者，大家可能没有我们观察仔细，我们认真研究了Qwen3-VL官方仓库的所有展出的测试用例，其中Qwen第一个展示的，就是Android use，整个模型能够通过观察整个界面和给定的指令，通过Reasoning，定位到操作的位置，做出相应的行为，并完成整体的操作。

结合上最近的新闻，阿里的夸克再曝新动作，提前透露了“C计划”，准备打造一个专门的AI浏览器。我相信，这个浏览器肯定不会像现在ChatGPT Atlas或者豆包这么简单，直接把浏览器做成对话式机器人入口，而会是一个更有想象力的方式。我认为应该是一个**全面的自动化Agent，不是改变人的使用习惯，把搜索变成对话，而是模拟人进行搜索，并以搜索关键词作为唯一目标，产出搜索结果**。

例如，你要定机票，这个智能化AI浏览器，应该可以直接操作一个普通的搜索界面，然后依次点开携程、去哪儿等网站，对比价格后进行提议，并等待用户审核。这个是最像人，也最轻松，使用VLM就能搞定，不用让所有企业都兼容一个Agent Protocol。用户说要定从A到B的机票，整个夸克AI浏览器就开始像人一样执行，用户躺着看就行了。



## 下一步

作为从业者，可以思考下VLM的Agent场景，如何能在有图片的情况下支持Long Context训练，是一个非常有意思的话题，也期待夸克能给大家打个样，展示下真正的AI浏览器。